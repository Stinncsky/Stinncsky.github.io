<!DOCTYPE html><html lang="zh-CN"><head><link href="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/css/index.css" rel="stylesheet"><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/atom.xml"><link rel="alternate" type="application/json" title="漠溟绽灵的小站" href="https://stinncsky.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="深度学习,cs231n,CNN,卷积神经网络,PyTorch,CIFAR-10,计算机视觉"><link rel="canonical" href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9APyTorch-on-CIFAR-10/"><title>cs231n的assignment2：PyTorch on CIFAR-10 - assignment2 - cs231n - 代码实现 - 机器学习 - 学习笔记 | Stinnc = 漠溟绽灵的小站 = 谁知道里面有什么</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">cs231n的assignment2：PyTorch on CIFAR-10</h1><div class="meta"><span class="item" title="创建时间：2025-08-05 20:39:31"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-08-05T20:39:31+08:00">2025-08-05</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>13k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>12 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Stinnc</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="/images/132325152_p0_master1200.jpg"></li><li class="item" data-background-image="/images/132357959_p0_master1200.jpg"></li><li class="item" data-background-image="/images/132363104_p0_master1200.jpg"></li><li class="item" data-background-image="/images/0ea650dd052f04785575d2e5feb6e9ea17c3d01f.jpg@1192w.avif"></li><li class="item" data-background-image="/images/8b6d46ef3b7137bb49aade189211a4b96b906ca9.jpg"></li><li class="item" data-background-image="/images/132319858_p0_master1200.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/" itemprop="item" rel="index" title="分类于 学习笔记"><span itemprop="name">学习笔记</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/" itemprop="item" rel="index" title="分类于 代码实现"><span itemprop="name">代码实现</span></a><meta itemprop="position" content="3"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/cs231n/" itemprop="item" rel="index" title="分类于 cs231n"><span itemprop="name">cs231n</span></a><meta itemprop="position" content="4"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/cs231n/assignment2/" itemprop="item" rel="index" title="分类于 assignment2"><span itemprop="name">assignment2</span></a><meta itemprop="position" content="5"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9APyTorch-on-CIFAR-10/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/hdd.jpg"><meta itemprop="name" content="漠溟绽灵Stinnc"><meta itemprop="description" content="谁知道里面有什么, 嘿嘿嘿~~"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="漠溟绽灵的小站"></span><div class="body md" itemprop="articleBody"><h1 id="环境配置与数据预处理"><a class="anchor" href="#环境配置与数据预处理">#</a> 环境配置与数据预处理</h1><h2 id="gpu-设备初始化"><a class="anchor" href="#gpu-设备初始化">#</a> GPU 设备初始化</h2><p>在深度学习训练中，GPU 加速是必不可少的。PyTorch 提供了简洁的设备管理机制。</p><figure class="highlight python"><figcaption><span>GPU设备配置</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> sampler</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> T</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">USE_GPU = <span class="literal">True</span></span><br><span class="line">dtype = torch.float32 <span class="comment"># 使用float32类型进行计算</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_GPU <span class="keyword">and</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 控制训练损失打印频率的常量</span></span><br><span class="line">print_every = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;using device:&#x27;</span>, device)</span><br></pre></td></tr></table></figure><h2 id="cifar-10-数据加载与预处理"><a class="anchor" href="#cifar-10-数据加载与预处理">#</a> CIFAR-10 数据加载与预处理</h2><p>CIFAR-10 数据集包含 60,000 张 32x32 彩色图像，分为 10 个类别。数据预处理包括标准化操作，将像素值转换为零均值单位方差的分布。</p><figure class="highlight python"><figcaption><span>数据加载与预处理</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">NUM_TRAIN = <span class="number">49000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torchvision.transforms 提供数据预处理和数据增强工具</span></span><br><span class="line"><span class="comment"># 这里设置变换来预处理数据：减去RGB均值并除以标准差</span></span><br><span class="line"><span class="comment"># 我们使用硬编码的均值和标准差</span></span><br><span class="line">transform = T.Compose([</span><br><span class="line">                T.ToTensor(),</span><br><span class="line">                T.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>))</span><br><span class="line">            ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个数据分割设置Dataset对象；Datasets逐个加载训练样本</span></span><br><span class="line"><span class="comment"># 因此我们将每个Dataset包装在DataLoader中，它遍历Dataset并形成小批量</span></span><br><span class="line"><span class="comment"># 通过向DataLoader传递Sampler对象将CIFAR-10训练集分为训练集和验证集</span></span><br><span class="line">cifar10_train = dset.CIFAR10(<span class="string">&#x27;./cs231n/datasets&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                             transform=transform)</span><br><span class="line">loader_train = DataLoader(cifar10_train, batch_size=<span class="number">64</span>,</span><br><span class="line">                          sampler=sampler.SubsetRandomSampler(<span class="built_in">range</span>(NUM_TRAIN)))</span><br><span class="line"></span><br><span class="line">cifar10_val = dset.CIFAR10(<span class="string">&#x27;./cs231n/datasets&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>,</span><br><span class="line">                           transform=transform)</span><br><span class="line">loader_val = DataLoader(cifar10_val, batch_size=<span class="number">64</span>,</span><br><span class="line">                        sampler=sampler.SubsetRandomSampler(<span class="built_in">range</span>(NUM_TRAIN, <span class="number">50000</span>)))</span><br><span class="line"></span><br><span class="line">cifar10_test = dset.CIFAR10(<span class="string">&#x27;./cs231n/datasets&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                            transform=transform)</span><br><span class="line">loader_test = DataLoader(cifar10_test, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><div class="note info no-icon"><p>CIFAR-10 的标准化参数 <code>(0.4914, 0.4822, 0.4465)</code> 和 <code>(0.2023, 0.1994, 0.2010)</code> 分别是训练集在 RGB 三个通道上的均值和标准差。</p></div><h1 id="神经网络架构设计"><a class="anchor" href="#神经网络架构设计">#</a> 神经网络架构设计</h1><h2 id="常见架构模式"><a class="anchor" href="#常见架构模式">#</a> 常见架构模式</h2><p>卷积神经网络的设计通常遵循以下几种模式：</p><ul><li><code>[conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax 或 SVM]</code></li><li><code>[conv-relu-conv-relu-pool]xN -&gt; [affine]xM -&gt; [softmax 或 SVM]</code></li><li><code>[batchnorm-relu-conv]xN -&gt; [affine]xM -&gt; [softmax 或 SVM]</code></li></ul><details class="info"><summary>Global Average Pooling 概念详解</summary><div><p>Global Average Pooling (GAP) 是一种池化技术，用于替代传统的扁平化操作和全连接层。其核心思想是：对每个特征图进行全局平均池化，将二维特征图压缩为单个数值。</p><p><strong>工作原理：</strong></p><ol><li>输入特征图形状为 (batch_size, channels, height, width)</li><li>对每个通道的所有空间位置求平均值</li><li>输出形状为 (batch_size, channels, 1, 1)</li><li>最后重塑为 (batch_size, channels) 向量</li></ol><p><strong>优势：</strong></p><ul><li>显著减少参数数量，降低过拟合风险</li><li>保持特征图的语义信息</li><li>提高模型的泛化能力</li><li>Google 的 Inception 网络成功应用了这一技术</li></ul><p><strong>适用场景：</strong><br>当卷积层输出的特征图尺寸较小（如 7x7）时，使用 GAP 比传统的扁平化 + 全连接层更有效。</p></div></details><h2 id="张量形状操作"><a class="anchor" href="#张量形状操作">#</a> 张量形状操作</h2><p>在神经网络中，经常需要调整张量的形状以适配不同层的输入要求。</p><figure class="highlight python"><figcaption><span>张量扁平化操作</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.view(N, -<span class="number">1</span>)  <span class="comment"># 将 C * H * W 值&quot;扁平化&quot;为每个图像的单个向量</span></span><br></pre></td></tr></table></figure><h1 id="基础网络实现"><a class="anchor" href="#基础网络实现">#</a> 基础网络实现</h1><h2 id="二层全连接网络"><a class="anchor" href="#二层全连接网络">#</a> 二层全连接网络</h2><p>二层全连接网络是最简单的神经网络架构，适用于理解基本的前向传播过程。</p><figure class="highlight python"><figcaption><span>二层全连接网络实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F  <span class="comment"># 有用的无状态函数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">two_layer_fc</span>(<span class="params">x, params</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    全连接神经网络；架构为：</span></span><br><span class="line"><span class="string">    全连接层 -&gt; ReLU -&gt; 全连接层</span></span><br><span class="line"><span class="string">    注意此函数只定义前向传播；</span></span><br><span class="line"><span class="string">    PyTorch 会为我们处理反向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    网络输入将是数据的小批量，形状为</span></span><br><span class="line"><span class="string">    (N, d1, ..., dM)，其中 d1 * ... * dM = D。隐藏层将有 H 个单元，</span></span><br><span class="line"><span class="string">    输出层将为 C 个类别产生分数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - x: PyTorch张量，形状(N, d1, ..., dM)，给出输入数据的小批量</span></span><br><span class="line"><span class="string">    - params: PyTorch张量列表[w1, w2]，给出网络权重；</span></span><br><span class="line"><span class="string">      w1形状(D, H)，w2形状(H, C)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    - scores: PyTorch张量，形状(N, C)，给出输入数据x的分类分数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 首先扁平化图像</span></span><br><span class="line">    x = flatten(x)  <span class="comment"># 形状：[batch_size, C x H x W]</span></span><br><span class="line"></span><br><span class="line">    w1, w2 = params</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播：使用张量运算计算预测的y</span></span><br><span class="line">    <span class="comment"># 由于w1和w2设置了requires_grad=True，涉及这些张量的运算将导致</span></span><br><span class="line">    <span class="comment"># PyTorch构建计算图，允许自动计算梯度</span></span><br><span class="line">    <span class="comment"># 你也可以使用`.clamp(min=0)`，等价于F.relu()</span></span><br><span class="line">    x = F.relu(x.mm(w1))</span><br><span class="line">    x = x.mm(w2)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">two_layer_fc_test</span>():</span><br><span class="line">    hidden_layer_size = <span class="number">42</span></span><br><span class="line">    x = torch.zeros((<span class="number">64</span>, <span class="number">50</span>), dtype=dtype)  <span class="comment"># 小批量大小64，特征维度50</span></span><br><span class="line">    w1 = torch.zeros((<span class="number">50</span>, hidden_layer_size), dtype=dtype)</span><br><span class="line">    w2 = torch.zeros((hidden_layer_size, <span class="number">10</span>), dtype=dtype)</span><br><span class="line">    scores = two_layer_fc(x, [w1, w2])</span><br><span class="line">    <span class="built_in">print</span>(scores.size())  <span class="comment"># 你应该看到[64, 10]</span></span><br><span class="line"></span><br><span class="line">two_layer_fc_test()</span><br></pre></td></tr></table></figure><h2 id="三层卷积网络"><a class="anchor" href="#三层卷积网络">#</a> 三层卷积网络</h2><p>卷积神经网络通过卷积操作提取空间特征，是处理图像数据的标准架构。</p><figure class="highlight python"><figcaption><span>三层卷积网络实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">three_layer_convnet</span>(<span class="params">x, params</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    执行上面定义架构的三层卷积网络的前向传播。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - x: PyTorch张量，形状(N, 3, H, W)，给出图像的小批量</span></span><br><span class="line"><span class="string">    - params: PyTorch张量列表，给出网络的权重和偏置；应包含：</span></span><br><span class="line"><span class="string">      - conv_w1: PyTorch张量，形状(channel_1, 3, KH1, KW1)，第一卷积层权重</span></span><br><span class="line"><span class="string">      - conv_b1: PyTorch张量，形状(channel_1,)，第一卷积层偏置</span></span><br><span class="line"><span class="string">      - conv_w2: PyTorch张量，形状(channel_2, channel_1, KH2, KW2)，第二卷积层权重</span></span><br><span class="line"><span class="string">      - conv_b2: PyTorch张量，形状(channel_2,)，第二卷积层偏置</span></span><br><span class="line"><span class="string">      - fc_w: PyTorch张量，全连接层权重</span></span><br><span class="line"><span class="string">      - fc_b: PyTorch张量，全连接层偏置</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">    - scores: PyTorch张量，形状(N, C)，给出x的分类分数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params</span><br><span class="line">    scores = <span class="literal">None</span></span><br><span class="line">    <span class="comment">################################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> 实现三层卷积网络的前向传播                                                #</span></span><br><span class="line">    <span class="comment">################################################################################</span></span><br><span class="line">    x = F.conv2d(x, conv_w1, bias=conv_b1, padding=<span class="number">2</span>)</span><br><span class="line">    x = F.relu(x)</span><br><span class="line">    x = F.conv2d(x, conv_w2, bias=conv_b2, padding=<span class="number">1</span>)</span><br><span class="line">    x = F.relu(x)</span><br><span class="line">    x = flatten(x)</span><br><span class="line">    scores = x.mm(fc_w) + fc_b</span><br><span class="line">    <span class="comment">################################################################################</span></span><br><span class="line">    <span class="comment">#                                 代码结束                                      #</span></span><br><span class="line">    <span class="comment">################################################################################</span></span><br><span class="line">    <span class="keyword">return</span> scores</span><br></pre></td></tr></table></figure><h1 id="参数初始化与训练"><a class="anchor" href="#参数初始化与训练">#</a> 参数初始化与训练</h1><h2 id="权重初始化策略"><a class="anchor" href="#权重初始化策略">#</a> 权重初始化策略</h2><p>正确的权重初始化对神经网络训练至关重要。Kaiming 初始化是针对 ReLU 激活函数优化的初始化方法。</p><details class="info"><summary>Kaiming Normalization 详解</summary><div><p>Kaiming Normalization（也称为 He 初始化）是由何恺明等人提出的权重初始化方法，专门针对使用 ReLU 激活函数的神经网络。</p><p><strong>数学原理：</strong><br>权重从均值为 0、方差为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>2</mn><mtext>fan_in</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{2}{\text{fan\_in}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.407108em;vertical-align:-.5619999999999999em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.845108em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">fan_in</span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.5619999999999999em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span> 的正态分布中采样，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>fan_in</mtext></mrow><annotation encoding="application/x-tex">\text{fan\_in}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.00444em;vertical-align:-.31em"></span><span class="mord text"><span class="mord">fan_in</span></span></span></span></span> 是输入单元数。</p><p><strong>计算公式：</strong></p><ul kernel\_width=""><li input\_size="">全连接层：\text{fan\_in} = \text</li><li>卷积层：\text{fan\_in} = \text{in\_channels} \times \text{kernel\_height} \times \text</li></ul><p><strong>设计理念：</strong><br>该方法确保在使用 ReLU 激活函数时，每层的输出方差保持稳定，避免梯度消失或梯度爆炸问题。</p><p><strong>与其他初始化方法的区别：</strong></p><ul><li>Xavier/Glorot 初始化适用于 tanh 和 sigmoid 激活函数</li><li>Kaiming 初始化专门为 ReLU 激活函数设计，考虑了 ReLU 会将一半的输入置零的特性</li></ul></div></details><figure class="highlight python"><figcaption><span>权重初始化函数</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">random_weight</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建权重的随机张量；设置requires_grad=True意味着我们</span></span><br><span class="line"><span class="string">    想要在反向传播过程中计算这些张量的梯度。</span></span><br><span class="line"><span class="string">    我们使用Kaiming标准化：sqrt(2 / fan_in)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(shape) == <span class="number">2</span>:  <span class="comment"># 全连接层权重</span></span><br><span class="line">        fan_in = shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        fan_in = np.prod(shape[<span class="number">1</span>:]) <span class="comment"># 卷积权重[out_channel, in_channel, kH, kW]</span></span><br><span class="line">    <span class="comment"># randn是标准正态分布生成器</span></span><br><span class="line">    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(<span class="number">2.</span> / fan_in)</span><br><span class="line">    w.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> w</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">zero_weight</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.zeros(shape, device=device, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建形状为[3 x 5]的权重</span></span><br><span class="line"><span class="comment"># 如果使用GPU，你应该看到类型`torch.cuda.FloatTensor`</span></span><br><span class="line"><span class="comment"># 否则应该是`torch.FloatTensor`</span></span><br><span class="line">random_weight((<span class="number">3</span>, <span class="number">5</span>))</span><br></pre></td></tr></table></figure><h2 id="模型评估"><a class="anchor" href="#模型评估">#</a> 模型评估</h2><p>模型评估是训练过程中的重要环节，用于监控模型性能和防止过拟合。</p><figure class="highlight python"><figcaption><span>准确率检查函数</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_accuracy_part2</span>(<span class="params">loader, model_fn, params</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    检查分类模型的准确率。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - loader: 要检查的数据分割的DataLoader</span></span><br><span class="line"><span class="string">    - model_fn: 执行模型前向传播的函数，</span></span><br><span class="line"><span class="string">      签名为scores = model_fn(x, params)</span></span><br><span class="line"><span class="string">    - params: 给出模型参数的PyTorch张量列表</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：无，但打印模型的准确率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    split = <span class="string">&#x27;val&#x27;</span> <span class="keyword">if</span> loader.dataset.train <span class="keyword">else</span> <span class="string">&#x27;test&#x27;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Checking accuracy on the %s set&#x27;</span> % split)</span><br><span class="line">    num_correct, num_samples = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">            x = x.to(device=device, dtype=dtype)  <span class="comment"># 移动到设备，如GPU</span></span><br><span class="line">            y = y.to(device=device, dtype=torch.int64)</span><br><span class="line">            scores = model_fn(x, params)</span><br><span class="line">            _, preds = scores.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">            num_correct += (preds == y).<span class="built_in">sum</span>()</span><br><span class="line">            num_samples += preds.size(<span class="number">0</span>)</span><br><span class="line">        acc = <span class="built_in">float</span>(num_correct) / num_samples</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Got %d / %d correct (%.2f%%)&#x27;</span> % (num_correct, num_samples, <span class="number">100</span> * acc))</span><br></pre></td></tr></table></figure><div class="note info no-icon"><p><code>scores.max(1)</code> 返回一个二元组 <code>(values, indices)</code> ，其中 <code>indices</code> 包含每行最大值的索引，即预测的类别标签。</p></div><h2 id="训练循环实现"><a class="anchor" href="#训练循环实现">#</a> 训练循环实现</h2><p>训练循环是深度学习的核心过程，包括前向传播、损失计算、反向传播和参数更新。</p><figure class="highlight python"><figcaption><span>训练函数实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_part2</span>(<span class="params">model_fn, params, learning_rate</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    在CIFAR-10上训练模型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - model_fn: 执行模型前向传播的Python函数。</span></span><br><span class="line"><span class="string">      应有签名scores = model_fn(x, params)，其中x是图像数据的PyTorch张量，</span></span><br><span class="line"><span class="string">      params是给出模型权重的PyTorch张量列表，scores是形状(N, C)的PyTorch张量</span></span><br><span class="line"><span class="string">    - params: 给出模型权重的PyTorch张量列表</span></span><br><span class="line"><span class="string">    - learning_rate: SGD使用的学习率Python标量</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：无</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> t, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_train):</span><br><span class="line">        <span class="comment"># 将数据移动到合适的设备（GPU或CPU）</span></span><br><span class="line">        x = x.to(device=device, dtype=dtype)</span><br><span class="line">        y = y.to(device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播：计算分数和损失</span></span><br><span class="line">        scores = model_fn(x, params)</span><br><span class="line">        loss = F.cross_entropy(scores, y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播：PyTorch找出计算图中哪些张量设置了requires_grad=True</span></span><br><span class="line">        <span class="comment"># 并使用反向传播计算损失相对于这些张量的梯度，</span></span><br><span class="line">        <span class="comment"># 并将梯度存储在每个张量的.grad属性中</span></span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新参数。我们不想通过参数更新进行反向传播，</span></span><br><span class="line">        <span class="comment"># 因此在torch.no_grad()上下文管理器下执行更新</span></span><br><span class="line">        <span class="comment"># 以防止构建计算图</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> params:</span><br><span class="line">                w -= learning_rate * w.grad</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 运行反向传播后手动将梯度清零</span></span><br><span class="line">                w.grad.zero_()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> t % print_every == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Iteration %d, loss = %.4f&#x27;</span> % (t, loss.item()))</span><br><span class="line">            check_accuracy_part2(loader_val, model_fn, params)</span><br><span class="line">            <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><h1 id="pytorch-module-api"><a class="anchor" href="#pytorch-module-api">#</a> PyTorch Module API</h1><h2 id="module-api-基础"><a class="anchor" href="#module-api-基础">#</a> Module API 基础</h2><p>PyTorch Module API 提供了更高级的抽象，简化了神经网络的定义和训练过程。</p><figure class="highlight python"><figcaption><span>二层全连接网络Module实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TwoLayerFC</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment"># 将层对象分配给类属性</span></span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(input_size, hidden_size)</span><br><span class="line">        <span class="comment"># nn.init包含便利的初始化方法</span></span><br><span class="line">        <span class="comment"># http://pytorch.org/docs/master/nn.html#torch-nn-init</span></span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.fc1.weight)</span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(hidden_size, num_classes)</span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.fc2.weight)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># forward始终定义连接性</span></span><br><span class="line">        x = flatten(x)</span><br><span class="line">        scores = <span class="variable language_">self</span>.fc2(F.relu(<span class="variable language_">self</span>.fc1(x)))</span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_TwoLayerFC</span>():</span><br><span class="line">    input_size = <span class="number">50</span></span><br><span class="line">    x = torch.zeros((<span class="number">64</span>, input_size), dtype=dtype)  <span class="comment"># 小批量大小64，特征维度50</span></span><br><span class="line">    model = TwoLayerFC(input_size, <span class="number">42</span>, <span class="number">10</span>)</span><br><span class="line">    scores = model(x)</span><br><span class="line">    <span class="built_in">print</span>(scores.size())  <span class="comment"># 你应该看到[64, 10]</span></span><br><span class="line">test_TwoLayerFC()</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>三层卷积网络Module实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ThreeLayerConvNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, channel_1, channel_2, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 设置三层卷积网络所需的层                                           #</span></span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="variable language_">self</span>.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=channel_1, kernel_size=<span class="number">5</span>, padding=<span class="number">2</span>)</span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.conv1.weight)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = nn.Conv2d(in_channels=channel_1, out_channels=channel_2, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.conv2.weight)</span><br><span class="line">        <span class="variable language_">self</span>.fc = nn.Linear(channel_2 * <span class="number">32</span> * <span class="number">32</span>, num_classes)</span><br><span class="line">        nn.init.kaiming_normal_(<span class="variable language_">self</span>.fc.weight)</span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="comment">#                          代码结束                                     #</span></span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> 实现3层卷积网络的前向函数                                          #</span></span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        out = <span class="variable language_">self</span>.conv1(x)</span><br><span class="line">        out = <span class="variable language_">self</span>.conv2(F.relu(out))</span><br><span class="line">        scores = <span class="variable language_">self</span>.fc(flatten(F.relu(out)))</span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="comment">#                             代码结束                                  #</span></span><br><span class="line">        <span class="comment">########################################################################</span></span><br><span class="line">        <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_ThreeLayerConvNet</span>():</span><br><span class="line">    x = torch.zeros((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>), dtype=dtype)  <span class="comment"># 小批量大小64，图像尺寸[3, 32, 32]</span></span><br><span class="line">    model = ThreeLayerConvNet(in_channel=<span class="number">3</span>, channel_1=<span class="number">12</span>, channel_2=<span class="number">8</span>, num_classes=<span class="number">10</span>)</span><br><span class="line">    scores = model(x)</span><br><span class="line">    <span class="built_in">print</span>(scores.size())  <span class="comment"># 你应该看到[64, 10]</span></span><br><span class="line">test_ThreeLayerConvNet()</span><br></pre></td></tr></table></figure><h2 id="高级训练实现"><a class="anchor" href="#高级训练实现">#</a> 高级训练实现</h2><p>使用 Module API 的训练过程更加简洁和标准化。</p><figure class="highlight python"><figcaption><span>Module API训练函数</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_accuracy_part34</span>(<span class="params">loader, model</span>):</span><br><span class="line">    <span class="keyword">if</span> loader.dataset.train:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Checking accuracy on validation set&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Checking accuracy on test set&#x27;</span>)</span><br><span class="line">    num_correct = <span class="number">0</span></span><br><span class="line">    num_samples = <span class="number">0</span></span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> loader:</span><br><span class="line">            x = x.to(device=device, dtype=dtype)  <span class="comment"># 移动到设备，如GPU</span></span><br><span class="line">            y = y.to(device=device, dtype=torch.long)</span><br><span class="line">            scores = model(x)</span><br><span class="line">            _, preds = scores.<span class="built_in">max</span>(<span class="number">1</span>)</span><br><span class="line">            num_correct += (preds == y).<span class="built_in">sum</span>()</span><br><span class="line">            num_samples += preds.size(<span class="number">0</span>)</span><br><span class="line">        acc = <span class="built_in">float</span>(num_correct) / num_samples</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Got %d / %d correct (%.2f)&#x27;</span> % (num_correct, num_samples, <span class="number">100</span> * acc))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_part34</span>(<span class="params">model, optimizer, epochs=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    使用PyTorch Module API在CIFAR-10上训练模型。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    输入：</span></span><br><span class="line"><span class="string">    - model: 要训练的PyTorch Module</span></span><br><span class="line"><span class="string">    - optimizer: 用于训练模型的Optimizer对象</span></span><br><span class="line"><span class="string">    - epochs: 训练的轮数（可选）</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    返回：无，但在训练期间打印模型准确率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model = model.to(device=device)  <span class="comment"># 将模型参数移动到CPU/GPU</span></span><br><span class="line">    <span class="keyword">for</span> e <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        <span class="keyword">for</span> t, (x, y) <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader_train):</span><br><span class="line">            model.train()  <span class="comment"># 将模型置于训练模式</span></span><br><span class="line">            x = x.to(device=device, dtype=dtype)  <span class="comment"># 移动到设备，如GPU</span></span><br><span class="line">            y = y.to(device=device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">            scores = model(x)</span><br><span class="line">            loss = F.cross_entropy(scores, y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 清零优化器将更新的所有变量的梯度</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 反向传播：计算损失相对于模型每个参数的梯度</span></span><br><span class="line">            loss.backward()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用反向传播计算的梯度实际更新模型参数</span></span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> t % print_every == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;Iteration %d, loss = %.4f&#x27;</span> % (t, loss.item()))</span><br><span class="line">                check_accuracy_part34(loader_val, model)</span><br><span class="line">                <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><h2 id="优化器使用"><a class="anchor" href="#优化器使用">#</a> 优化器使用</h2><p>PyTorch 提供了多种优化算法，SGD 是最基础和常用的优化器之一。</p><figure class="highlight python"><figcaption><span>优化器配置示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hidden_layer_size = <span class="number">4000</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">model = TwoLayerFC(<span class="number">3</span> * <span class="number">32</span> * <span class="number">32</span>, hidden_layer_size, <span class="number">10</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">train_part34(model, optimizer)</span><br></pre></td></tr></table></figure><h1 id="sequential-api-高级应用"><a class="anchor" href="#sequential-api-高级应用">#</a> Sequential API 高级应用</h1><h2 id="sequential-模型构建"><a class="anchor" href="#sequential-模型构建">#</a> Sequential 模型构建</h2><p>Sequential API 提供了最简洁的网络定义方式，特别适合线性堆叠的网络架构。</p><details class="info"><summary>Strided Convolution 概念详解</summary><div><p>Strided Convolution（步幅卷积）是卷积操作的一种变体，通过设置步幅参数控制卷积核在输入特征图上的移动距离。</p><p><strong>基本概念：</strong></p><ul><li>步幅（stride）决定了卷积核每次移动的像素数</li><li>stride=1：卷积核每次移动 1 个像素（标准卷积）</li><li>stride=2：卷积核每次移动 2 个像素，输出尺寸减半</li></ul><p><strong>数学关系：</strong><br>输出尺寸 = <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">⌊</mo><mfrac><mrow><mtext>输入尺寸</mtext><mo>+</mo><mn>2</mn><mo>×</mo><mtext>填充</mtext><mo>−</mo><mtext>卷积核尺寸</mtext></mrow><mtext>步幅</mtext></mfrac><mo stretchy="false">⌋</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lfloor\frac{\text{输入尺寸} + 2 \times \text{填充} - \text{卷积核尺寸}}{\text{步幅}}\rfloor + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.217331em;vertical-align:-.345em"></span><span class="mopen">⌊</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.872331em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord cjk_fallback mtight">步幅</span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord cjk_fallback mtight">输入尺寸</span></span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord cjk_fallback mtight">填充</span></span><span class="mbin mtight">−</span><span class="mord text mtight"><span class="mord cjk_fallback mtight">卷积核尺寸</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">⌋</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span></p><p><strong>应用场景：</strong></p><ol><li><strong>下采样</strong>：替代池化层进行特征图尺寸缩减</li><li><strong>计算效率</strong>：减少计算量和内存使用</li><li><strong>特征提取</strong>：在降低分辨率的同时提取关键特征</li></ol><p><strong>优势与劣势：</strong></p><ul><li>优势：参数可学习，比固定的池化操作更灵活</li><li>劣势：可能丢失部分空间信息，需要合理设计网络深度</li></ul></div></details><figure class="highlight python"><figcaption><span>Sequential API示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们需要将`flatten`函数包装在模块中，以便在nn.Sequential中堆叠它</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Flatten</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> flatten(x)</span><br><span class="line"></span><br><span class="line">hidden_layer_size = <span class="number">4000</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">3</span> * <span class="number">32</span> * <span class="number">32</span>, hidden_layer_size),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(hidden_layer_size, <span class="number">10</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以在optim.SGD中使用Nesterov动量</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate,</span><br><span class="line">                     momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_part34(model, optimizer)</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>Sequential卷积网络实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">channel_1 = <span class="number">32</span></span><br><span class="line">channel_2 = <span class="number">16</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line"></span><br><span class="line">model = <span class="literal">None</span></span><br><span class="line">optimizer = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> 使用Sequential API重写第三部分的2层带偏置卷积网络                            #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">3</span>,channel_1,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(channel_1,channel_2,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    Flatten(),</span><br><span class="line">    nn.Linear(<span class="number">16</span>*<span class="number">32</span>*<span class="number">32</span>,<span class="number">10</span>)</span><br><span class="line">)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=learning_rate,</span><br><span class="line">                     momentum=<span class="number">0.9</span>, nesterov=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 代码结束                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line">train_part34(model, optimizer)</span><br></pre></td></tr></table></figure><div class="tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 深度学习</a> <a href="/tags/cs231n/" rel="tag"><i class="ic i-tag"></i> cs231n</a> <a href="/tags/CNN/" rel="tag"><i class="ic i-tag"></i> CNN</a> <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="ic i-tag"></i> 卷积神经网络</a> <a href="/tags/PyTorch/" rel="tag"><i class="ic i-tag"></i> PyTorch</a> <a href="/tags/CIFAR-10/" rel="tag"><i class="ic i-tag"></i> CIFAR-10</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"><i class="ic i-tag"></i> 计算机视觉</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-08-10 21:16:48" itemprop="dateModified" datetime="2025-08-10T21:16:48+08:00">2025-08-10</time> </span><span id="cs231n的assignment2：PyTorch-on-CIFAR-10/" class="item leancloud_visitors" data-flag-title="cs231n的assignment2：PyTorch on CIFAR-10" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="漠溟绽灵Stinnc 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.jpg" alt="漠溟绽灵Stinnc 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>漠溟绽灵Stinnc <i class="ic i-at"><em>@</em></i>漠溟绽灵的小站</li><li class="link"><strong>本文链接：</strong> <a href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9APyTorch-on-CIFAR-10/" title="cs231n的assignment2：PyTorch on CIFAR-10">https://stinncsky.github.io/cs231n的assignment2：PyTorch-on-CIFAR-10/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3477-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE-II/" itemprop="url" rel="prev" data-background-image="&#x2F;images&#x2F;0ea650dd052f04785575d2e5feb6e9ea17c3d01f.jpg@1192w.avif" title="力扣每日一题：3477. 水果成篮 II"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 每日一题</span><h3>力扣每日一题：3477. 水果成篮 II</h3></a></div><div class="item right"><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3479-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE-III/" itemprop="url" rel="next" data-background-image="&#x2F;images&#x2F;132357959_p0_master1200.jpg" title="力扣每日一题：3479. 水果成篮 III"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 算法题解</span><h3>力扣每日一题：3479. 水果成篮 III</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%E4%B8%8E%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">环境配置与数据预处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#gpu-%E8%AE%BE%E5%A4%87%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">1.1.</span> <span class="toc-text">GPU 设备初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cifar-10-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E4%B8%8E%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">1.2.</span> <span class="toc-text">CIFAR-10 数据加载与预处理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">2.</span> <span class="toc-text">神经网络架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E6%9E%B6%E6%9E%84%E6%A8%A1%E5%BC%8F"><span class="toc-number">2.1.</span> <span class="toc-text">常见架构模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">张量形状操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">基础网络实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E5%B1%82%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C"><span class="toc-number">3.1.</span> <span class="toc-text">二层全连接网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="toc-number">3.2.</span> <span class="toc-text">三层卷积网络</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%AE%AD%E7%BB%83"><span class="toc-number">4.</span> <span class="toc-text">参数初始化与训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.</span> <span class="toc-text">权重初始化策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0"><span class="toc-number">4.2.</span> <span class="toc-text">模型评估</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%BE%AA%E7%8E%AF%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.3.</span> <span class="toc-text">训练循环实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-module-api"><span class="toc-number">5.</span> <span class="toc-text">PyTorch Module API</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#module-api-%E5%9F%BA%E7%A1%80"><span class="toc-number">5.1.</span> <span class="toc-text">Module API 基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.2.</span> <span class="toc-text">高级训练实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%E4%BD%BF%E7%94%A8"><span class="toc-number">5.3.</span> <span class="toc-text">优化器使用</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sequential-api-%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8"><span class="toc-number">6.</span> <span class="toc-text">Sequential API 高级应用</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">6.1.</span> <span class="toc-text">Sequential 模型构建</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ABatchNormalization/" rel="bookmark" title="cs231n的assignment2：BatchNormalization">cs231n的assignment2：BatchNormalization</a></li><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ADropout/" rel="bookmark" title="cs231n的assignment2：Dropout">cs231n的assignment2：Dropout</a></li><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/" rel="bookmark" title="cs231n的assignment2：CNN">cs231n的assignment2：CNN</a></li><li class="active"><a href="/cs231n%E7%9A%84assignment2%EF%BC%9APyTorch-on-CIFAR-10/" rel="bookmark" title="cs231n的assignment2：PyTorch on CIFAR-10">cs231n的assignment2：PyTorch on CIFAR-10</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="漠溟绽灵Stinnc" data-src="/images/hdd.jpg"><p class="name" itemprop="name">漠溟绽灵Stinnc</p><div class="description" itemprop="description">嘿嘿嘿~~</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">55</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">12</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">133</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0aW5uY3NreQ==" title="https:&#x2F;&#x2F;github.com&#x2F;stinncsky"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTExMDY5NDgzNA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;110694834"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3477-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE-II/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3479-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE-III/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/" title="分类于 代码实现">代码实现</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/" title="分类于 cs231n">cs231n</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/assignment2/" title="分类于 assignment2">assignment2</a></div><span><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/" title="cs231n的assignment2：CNN">cs231n的assignment2：CNN</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/" title="分类于 代码实现">代码实现</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/Happy-LLM/" title="分类于 Happy-LLM">Happy-LLM</a></div><span><a href="/study-notes/machine-learning/code-implementation/Transformer%E7%9A%84%E5%AE%9E%E7%8E%B0/" title="Transformer的实现">Transformer的实现</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A2163-%E5%88%A0%E9%99%A4%E5%85%83%E7%B4%A0%E5%90%8E%E5%92%8C%E7%9A%84%E6%9C%80%E5%B0%8F%E5%B7%AE%E5%80%BC/" title="力扣每日一题：2163. 删除元素后和的最小差值">力扣每日一题：2163. 删除元素后和的最小差值</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/" title="分类于 代码实现">代码实现</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/" title="分类于 cs231n">cs231n</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/assignment1/" title="分类于 assignment1">assignment1</a></div><span><a href="/study-notes/machine-learning/code-implementation/cs231n%E7%9A%84assignment1%EF%BC%9AKNN/" title="cs231n的assignment1：KNN">cs231n的assignment1：KNN</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/study-notes/machine-learning/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E8%AF%86/" title="机器学习初识">机器学习初识</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/study-notes/machine-learning/%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%88%86%E7%B1%BB%EF%BC%9ASoftmax%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5/" title="从回归到分类：Softmax与交叉熵">从回归到分类：Softmax与交叉熵</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3363-%E6%9C%80%E5%A4%9A%E5%8F%AF%E6%94%B6%E9%9B%86%E7%9A%84%E6%B0%B4%E6%9E%9C%E6%95%B0%E7%9B%AE/" title="力扣每日一题：3363. 最多可收集的水果数目">力扣每日一题：3363. 最多可收集的水果数目</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1957-%E5%88%A0%E9%99%A4%E5%AD%97%E7%AC%A6%E4%BD%BF%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8F%98%E5%A5%BD/" title="力扣每日一题：1957. 删除字符使字符串变好">力扣每日一题：1957. 删除字符使字符串变好</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1233-%E5%88%A0%E9%99%A4%E5%AD%90%E6%96%87%E4%BB%B6%E5%A4%B9/" title="力扣每日一题：1233. 删除子文件夹">力扣每日一题：1233. 删除子文件夹</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3440-%E9%87%8D%E6%96%B0%E5%AE%89%E6%8E%92%E4%BC%9A%E8%AE%AE%E5%BE%97%E5%88%B0%E6%9C%80%E5%A4%9A%E7%A9%BA%E4%BD%99%E6%97%B6%E9%97%B4-II/" title="力扣每日一题：3440. 重新安排会议得到最多空余时间 II">力扣每日一题：3440. 重新安排会议得到最多空余时间 II</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">漠溟绽灵Stinnc @ Stinnc</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">192k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:54</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"cs231n的assignment2：PyTorch-on-CIFAR-10/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/js/index.js"></script></body></html>