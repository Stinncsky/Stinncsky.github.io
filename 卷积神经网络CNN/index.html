<!DOCTYPE html><html lang="zh-CN"><head><link href="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/css/index.css" rel="stylesheet"><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/atom.xml"><link rel="alternate" type="application/json" title="漠溟绽灵的小站" href="https://stinncsky.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="神经网络,深度学习,CNN,卷积神经网络,图像识别,计算机视觉,特征提取,池化"><link rel="canonical" href="https://stinncsky.github.io/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"><title>卷积神经网络CNN - 机器学习 - 学习笔记 | Stinnc = 漠溟绽灵的小站 = 谁知道里面有什么</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">卷积神经网络CNN</h1><div class="meta"><span class="item" title="创建时间：2025-07-09 19:22:34"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-07-09T19:22:34+08:00">2025-07-09</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>3.7k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>3 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Stinnc</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="/images/0ea650dd052f04785575d2e5feb6e9ea17c3d01f.jpg@1192w.avif"></li><li class="item" data-background-image="/images/8b6d46ef3b7137bb49aade189211a4b96b906ca9.jpg"></li><li class="item" data-background-image="/images/132325152_p0_master1200.jpg"></li><li class="item" data-background-image="/images/bd33d1b08260ad781843ed6667db3356ca53e6c5.jpg"></li><li class="item" data-background-image="/images/132363104_p0_master1200.jpg"></li><li class="item" data-background-image="/images/132319858_p0_master1200.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/" itemprop="item" rel="index" title="分类于 学习笔记"><span itemprop="name">学习笔记</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://stinncsky.github.io/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/hdd.jpg"><meta itemprop="name" content="漠溟绽灵Stinnc"><meta itemprop="description" content="谁知道里面有什么, 嘿嘿嘿~~"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="漠溟绽灵的小站"></span><div class="body md" itemprop="articleBody"><h1 id="卷积神经网络基础概念"><a class="anchor" href="#卷积神经网络基础概念">#</a> 卷积神经网络基础概念</h1><p>** 卷积神经网络（Convolutional Neural Network, CNN）** 是一种专门用于处理具有网格状拓扑结构数据的深度学习架构，在<ins class="dot">图像分类</ins>、目标检测、图像分割等计算机视觉任务中表现卓越。</p><details class="info"><summary>关键概念：Tensor</summary><div><p>**Tensor（张量）** 是 CNN 中数据的基本表示形式，可以理解为多维数组：</p><ul><li><strong>0 维张量</strong>：标量（单个数值）</li><li><strong>1 维张量</strong>：向量（一维数组）</li><li><strong>2 维张量</strong>：矩阵（二维数组）</li><li><strong>3 维张量</strong>：如 RGB 图像（高度 × 宽度 × 通道数）</li><li><strong>4 维张量</strong>：批量图像数据（批量大小 × 高度 × 宽度 × 通道数）</li></ul><p>在 CNN 中，图像通常表示为 3 维张量，其中包含<strong>高度（Height）</strong>、** 宽度（Width）<strong>和</strong>颜色通道（Color Channels）** 三个维度。</p></div></details><h2 id="传统全连接网络的局限性"><a class="anchor" href="#传统全连接网络的局限性">#</a> 传统全连接网络的局限性</h2><p>传统的神经网络在处理图像时存在显著问题。以一张 100×100 像素的 RGB 彩色图像为例：</p><ul><li>图像维度：100 × 100 × 3 = 30,000 个像素值</li><li>如果使用 1000 个神经元的全连接层：需要 30,000 × 1000 = <strong>3×10^7 个权重参数</strong></li></ul><div class="note warning no-icon"><p>参数量过多导致：</p><ol><li><strong>过拟合风险</strong>：模型容易记住训练数据而非学习特征</li><li><strong>计算复杂度高</strong>：训练和推理速度慢</li><li><strong>内存占用大</strong>：存储和加载模型困难</li></ol></div><p>更重要的是，图像具有<strong>空间局部性</strong>特征 —— 相邻像素通常高度相关，而全连接网络无法有效利用这种空间结构信息。</p><h1 id="cnn的核心思想感受野与参数共享"><a class="anchor" href="#cnn的核心思想感受野与参数共享">#</a> CNN 的核心思想：感受野与参数共享</h1><h2 id="感受野机制"><a class="anchor" href="#感受野机制">#</a> 感受野机制</h2><p>CNN 的突破性创新在于引入了<ins class="dot">感受野</ins>（Receptive Field）概念，让每个神经元只关注图像的局部区域。</p><details class="success"><summary>感受野详解</summary><div><p>** 感受野（Receptive Field）** 是指神经网络中某个神经元能够 &quot;看到&quot; 的输入区域范围。在 CNN 中：</p><ul><li><strong>局部连接</strong>：每个神经元只与输入的一个小区域相连</li><li><strong>重叠覆盖</strong>：不同神经元的感受野可以重叠，确保信息不丢失</li><li><strong>多神经元协作</strong>：多个神经元可以负责同一个感受野，提取不同特征</li><li><strong>逐层扩大</strong>：随着网络深度增加，后层神经元的有效感受野逐渐扩大</li></ul><p>经典设置：感受野大小通常为 3×3 或 5×5，考虑所有颜色通道。</p></div></details><div class="note info no-icon"><p>实际上，我们<ins class="dot">只需要图像的部分特征就可以进行有效识别</ins>，比如识别猫咪时，我们主要关注耳朵、眼睛、胡须等局部特征，而不需要分析整张图片的每个像素。</p></div><h2 id="参数共享策略"><a class="anchor" href="#参数共享策略">#</a> 参数共享策略</h2><p>CNN 的第二个关键创新是<strong>参数共享</strong>：不同位置的感受野使用相同的权重参数。</p><figure class="highlight python"><figcaption><span>参数共享示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传统全连接：每个连接都有独立参数</span></span><br><span class="line">w1, w2, w3, ..., w_n  <span class="comment"># n个不同的权重</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CNN卷积：所有位置共享同一组参数  </span></span><br><span class="line">w_shared = [w1, w2, w3, ..., w_k]  <span class="comment"># 只有k个权重（k &lt;&lt; n）</span></span><br><span class="line"><span class="comment"># 这组权重在所有感受野位置重复使用</span></span><br></pre></td></tr></table></figure><p><strong>为什么可以共享参数？</strong></p><p>不同位置要检测的基本模式是相同的 —— 边缘、角点、纹理等特征在图像的任何位置都可能出现，因此使用相同的参数是合理的。</p><h1 id="卷积层cnn的核心组件"><a class="anchor" href="#卷积层cnn的核心组件">#</a> 卷积层：CNN 的核心组件</h1><p><strong>卷积层 = 感受野 + 参数共享</strong></p><details class="info"><summary>卷积核详解</summary><div><p>** 卷积核（Filter/Kernel）** 是 CNN 中的特征检测器：</p><ul><li><strong>结构</strong>：通常为 3×3、5×5 或 7×7 的小矩阵</li><li><strong>功能</strong>：检测特定的图像特征（如边缘、角点、纹理）</li><li><strong>参数</strong>：卷积核中的数值就是需要学习的权重参数</li><li><strong>数量</strong>：一个卷积层通常包含多个卷积核，每个检测不同类型的特征</li><li><strong>深度</strong>：卷积核的深度必须等于输入数据的通道数</li></ul></div></details><h2 id="卷积操作过程"><a class="anchor" href="#卷积操作过程">#</a> 卷积操作过程</h2><ol><li><strong>扫描过程</strong>：卷积核以指定的步长（stride）在输入图像上滑动</li><li><strong>特征提取</strong>：在每个位置进行元素相乘并求和</li><li><strong>生成特征图</strong>：每个卷积核扫过整张图后，生成一张<strong>特征图（Feature Map）</strong></li></ol><details class="warning"><summary>步长设置</summary><div><p>** 步长（Stride）** 控制卷积核的移动距离：</p><ul><li><strong>stride = 1</strong>：卷积核每次移动 1 个像素，特征图尺寸较大，信息保留更完整</li><li><strong>stride = 2</strong>：卷积核每次移动 2 个像素，特征图尺寸缩小一半，计算更高效</li></ul><p>经典设置通常使用 stride = 1 或 2。当卷积核移动超出图像边界时，可以使用 ** 填充（Padding）** 来处理边界问题。</p></div></details><h2 id="多层卷积的层次化特征学习"><a class="anchor" href="#多层卷积的层次化特征学习">#</a> 多层卷积的层次化特征学习</h2><figure class="highlight python"><figcaption><span>卷积层级示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一层卷积：检测基础特征</span></span><br><span class="line">input_shape: (H, W, <span class="number">3</span>)      <span class="comment"># 原始RGB图像</span></span><br><span class="line">filter_1: <span class="number">32</span>个<span class="number">3</span>×<span class="number">3</span>×<span class="number">3</span>卷积核    <span class="comment"># 输出32个特征图</span></span><br><span class="line">output_1: (H<span class="string">&#x27;, W&#x27;</span>, <span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层卷积：组合基础特征  </span></span><br><span class="line">filter_2: <span class="number">64</span>个<span class="number">3</span>×<span class="number">3</span>×<span class="number">32</span>卷积核   <span class="comment"># 处理32个输入通道</span></span><br><span class="line">output_2: (H<span class="string">&#x27;&#x27;</span>, W<span class="string">&#x27;&#x27;</span>, <span class="number">64</span>)    <span class="comment"># 输出64个更复杂的特征图</span></span><br></pre></td></tr></table></figure><div class="note success no-icon"><p><strong>层次化特征学习</strong>：</p><ul><li><strong>浅层</strong>：检测边缘、角点等基础特征</li><li><strong>中层</strong>：组合基础特征形成纹理、形状</li><li><strong>深层</strong>：形成高级语义特征，如物体部分</li></ul></div><h1 id="池化层降维与抗干扰"><a class="anchor" href="#池化层降维与抗干扰">#</a> 池化层：降维与抗干扰</h1><p>在卷积层之后，通常会添加 ** 池化层（Pooling Layer）** 来减少特征图的空间维度。</p><details class="primary"><summary>最大池化详解</summary><div><p>** 最大池化（Max Pooling）** 是最常用的池化操作：</p><ul><li><strong>操作</strong>：在指定区域内选择最大值作为输出</li><li><strong>窗口大小</strong>：通常为 2×2 或 3×3</li><li><strong>效果</strong>：<ul><li>减少参数数量和计算量</li><li>提供平移不变性（轻微位移不影响结果）</li><li>保留最显著的特征信息</li></ul></li></ul><p><strong>示例</strong>：2×2 最大池化将 4 个值缩减为 1 个值，特征图尺寸减半。</p></div></details><figure class="highlight python"><figcaption><span>池化层示例 mark:3</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2×2最大池化，步长为2</span></span><br><span class="line">max_pool = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 输入: (batch_size, channels, 28, 28)</span></span><br><span class="line"><span class="comment"># 输出: (batch_size, channels, 14, 14)</span></span><br></pre></td></tr></table></figure><div class="note warning no-icon"><p>池化操作的权衡：</p><ul><li><strong>优点</strong>：减少计算量，增强鲁棒性</li><li><strong>缺点</strong>：会丢失部分空间信息和精度</li></ul></div><h1 id="cnn完整架构流程"><a class="anchor" href="#cnn完整架构流程">#</a> CNN 完整架构流程</h1><h2 id="标准cnn工作流程"><a class="anchor" href="#标准cnn工作流程">#</a> 标准 CNN 工作流程</h2><figure class="highlight python"><figcaption><span>CNN完整流程 mark:5,10,15</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 输入层</span></span><br><span class="line">input_image = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)  <span class="comment"># RGB图像</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 卷积 + 池化模块（可重复多次）</span></span><br><span class="line">conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">3</span>, padding=<span class="number">1</span>)     <span class="comment"># 第一个卷积层</span></span><br><span class="line">pool1 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)                 <span class="comment"># 第一个池化层</span></span><br><span class="line">conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, padding=<span class="number">1</span>)    <span class="comment"># 第二个卷积层  </span></span><br><span class="line">pool2 = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)                 <span class="comment"># 第二个池化层</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 展平层（Flatten）</span></span><br><span class="line">flatten = nn.Flatten()                      <span class="comment"># 将3D特征图拉直为1D向量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 全连接层</span></span><br><span class="line">fc1 = nn.Linear(<span class="number">64</span> * <span class="number">56</span> * <span class="number">56</span>, <span class="number">512</span>)         <span class="comment"># 第一个全连接层</span></span><br><span class="line">fc2 = nn.Linear(<span class="number">512</span>, <span class="number">10</span>)                   <span class="comment"># 输出层（如10分类）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 激活函数和最终输出</span></span><br><span class="line">softmax = nn.Softmax(dim=<span class="number">1</span>)                <span class="comment"># 分类概率输出</span></span><br></pre></td></tr></table></figure><details class="info"><summary>展平操作详解</summary><div><p>** 展平（Flatten）** 是连接卷积部分和全连接部分的关键步骤：</p><ul><li><strong>目的</strong>：将多维特征图转换为一维向量</li><li><strong>过程</strong>：将 (批量大小，通道数，高度，宽度) 的 4D 张量重塑为 (批量大小，特征数) 的 2D 张量</li><li><strong>位置</strong>：通常放在最后一个池化层之后，第一个全连接层之前</li></ul><p><strong>示例</strong>：(32, 128, 7, 7) → (32, 6272)，其中 6272 = 128 × 7 × 7</p></div></details><h2 id="数据预处理one-hot编码"><a class="anchor" href="#数据预处理one-hot编码">#</a> 数据预处理：One-Hot 编码</h2><p>在分类任务中，CNN 通常使用<strong> One-Hot 编码</strong>来表示类别标签：</p><figure class="highlight python"><figcaption><span>One-Hot编码示例</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始标签（如10类分类）</span></span><br><span class="line">label = <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># One-Hot编码</span></span><br><span class="line">one_hot = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment">#          类别: 0  1  2  3  4  5  6  7  8  9</span></span><br></pre></td></tr></table></figure><div class="note info no-icon"><p>One-Hot 编码的优势：</p><ul><li>避免类别间的大小关系假设</li><li>便于计算交叉熵损失函数</li><li>输出概率分布更加直观</li></ul></div><h1 id="数据增强处理尺寸变化"><a class="anchor" href="#数据增强处理尺寸变化">#</a> 数据增强：处理尺寸变化</h1><p>CNN 对输入图像尺寸较为敏感，无法直接处理不同大小的图像。因此需要进行<strong>数据增强（Data Augmentation）</strong>。</p><div class="tab" data-id="augmentation" data-title="常见数据增强技术"><p><strong>几何变换</strong>：</p><ul><li>随机裁剪和缩放</li><li>旋转和翻转</li><li>平移和错切</li></ul><p><strong>颜色变换</strong>：</p><ul><li>亮度和对比度调整</li><li>色相和饱和度变化</li><li>随机擦除部分区域</li></ul></div><div class="tab" data-id="benefits" data-title="数据增强的作用"><p><strong>提升模型泛化能力</strong>：</p><ul><li>增加训练数据的多样性</li><li>减少过拟合风险</li><li>提高对图像变化的鲁棒性</li></ul><p><strong>统一输入格式</strong>：</p><ul><li>将不同尺寸图像调整为固定大小</li><li>保持模型架构的一致性</li></ul></div><figure class="highlight python"><figcaption><span>数据增强代码示例 mark:2-4</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.RandomResizedCrop(<span class="number">224</span>),      <span class="comment"># 随机裁剪到224×224</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),      <span class="comment"># 随机水平翻转</span></span><br><span class="line">    transforms.ColorJitter(<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>), <span class="comment"># 颜色抖动</span></span><br><span class="line">    transforms.ToTensor(),                  <span class="comment"># 转换为张量</span></span><br><span class="line">    transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], </span><br><span class="line">                        [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]) <span class="comment"># 标准化</span></span><br><span class="line">])</span><br></pre></td></tr></table></figure><h1 id="cnn的优势与应用"><a class="anchor" href="#cnn的优势与应用">#</a> CNN 的优势与应用</h1><p>CNN 相比传统方法具有以下显著优势：</p><ul class="task-list success"><li class="task-list-item"><input type="checkbox" id="cbx_0" checked disabled><label for="cbx_0"><strong>参数效率</strong>：通过参数共享大幅减少模型参数</label></li><li class="task-list-item"><input type="checkbox" id="cbx_1" checked disabled><label for="cbx_1"><strong>空间不变性</strong>：对图像中物体的位置变化具有鲁棒性</label></li><li class="task-list-item"><input type="checkbox" id="cbx_2" checked disabled><label for="cbx_2"><strong>层次化特征学习</strong>：自动学习从低级到高级的特征表示</label></li><li class="task-list-item"><input type="checkbox" id="cbx_3" checked disabled><label for="cbx_3"><strong>端到端训练</strong>：可以直接从原始像素学习到最终分类结果</label></li></ul><div class="tags"><a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="ic i-tag"></i> 神经网络</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 深度学习</a> <a href="/tags/CNN/" rel="tag"><i class="ic i-tag"></i> CNN</a> <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="ic i-tag"></i> 卷积神经网络</a> <a href="/tags/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/" rel="tag"><i class="ic i-tag"></i> 图像识别</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"><i class="ic i-tag"></i> 计算机视觉</a> <a href="/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/" rel="tag"><i class="ic i-tag"></i> 特征提取</a> <a href="/tags/%E6%B1%A0%E5%8C%96/" rel="tag"><i class="ic i-tag"></i> 池化</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-07-09 23:36:06" itemprop="dateModified" datetime="2025-07-09T23:36:06+08:00">2025-07-09</time> </span><span id="卷积神经网络CNN/" class="item leancloud_visitors" data-flag-title="卷积神经网络CNN" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="漠溟绽灵Stinnc 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.jpg" alt="漠溟绽灵Stinnc 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>漠溟绽灵Stinnc <i class="ic i-at"><em>@</em></i>漠溟绽灵的小站</li><li class="link"><strong>本文链接：</strong> <a href="https://stinncsky.github.io/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/" title="卷积神经网络CNN">https://stinncsky.github.io/卷积神经网络CNN/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/" itemprop="url" rel="prev" data-background-image="&#x2F;images&#x2F;132357959_p0_master1200.jpg" title="批量归一化"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> 机器学习</span><h3>批量归一化</h3></a></div><div class="item right"><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3440-%E9%87%8D%E6%96%B0%E5%AE%89%E6%8E%92%E4%BC%9A%E8%AE%AE%E5%BE%97%E5%88%B0%E6%9C%80%E5%A4%9A%E7%A9%BA%E4%BD%99%E6%97%B6%E9%97%B4-II/" itemprop="url" rel="next" data-background-image="&#x2F;images&#x2F;132319858_p0_master1200.jpg" title="力扣每日一题：3440. 重新安排会议得到最多空余时间 II"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 每日一题</span><h3>力扣每日一题：3440. 重新安排会议得到最多空余时间 II</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">卷积神经网络基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E5%85%A8%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text">传统全连接网络的局限性</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E6%84%9F%E5%8F%97%E9%87%8E%E4%B8%8E%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB"><span class="toc-number">2.</span> <span class="toc-text">CNN 的核心思想：感受野与参数共享</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E5%8F%97%E9%87%8E%E6%9C%BA%E5%88%B6"><span class="toc-number">2.1.</span> <span class="toc-text">感受野机制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB%E7%AD%96%E7%95%A5"><span class="toc-number">2.2.</span> <span class="toc-text">参数共享策略</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82cnn%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">3.</span> <span class="toc-text">卷积层：CNN 的核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C%E8%BF%87%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">卷积操作过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%9A%84%E5%B1%82%E6%AC%A1%E5%8C%96%E7%89%B9%E5%BE%81%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.2.</span> <span class="toc-text">多层卷积的层次化特征学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E9%99%8D%E7%BB%B4%E4%B8%8E%E6%8A%97%E5%B9%B2%E6%89%B0"><span class="toc-number">4.</span> <span class="toc-text">池化层：降维与抗干扰</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn%E5%AE%8C%E6%95%B4%E6%9E%B6%E6%9E%84%E6%B5%81%E7%A8%8B"><span class="toc-number">5.</span> <span class="toc-text">CNN 完整架构流程</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86cnn%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.1.</span> <span class="toc-text">标准 CNN 工作流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86one-hot%E7%BC%96%E7%A0%81"><span class="toc-number">5.2.</span> <span class="toc-text">数据预处理：One-Hot 编码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%E5%A4%84%E7%90%86%E5%B0%BA%E5%AF%B8%E5%8F%98%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">数据增强：处理尺寸变化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn%E7%9A%84%E4%BC%98%E5%8A%BF%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="toc-number">7.</span> <span class="toc-text">CNN 的优势与应用</span></a></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E8%AF%86/" rel="bookmark" title="机器学习初识">机器学习初识</a></li><li><a href="/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E9%9E%8D%E7%82%B9%E4%B8%8E%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" rel="bookmark" title="模型训练问题：过拟合、鞍点与优化算法">模型训练问题：过拟合、鞍点与优化算法</a></li><li><a href="/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%9A%E4%BB%8EAdaGrad%E5%88%B0Adam/" rel="bookmark" title="自适应学习率：从AdaGrad到Adam">自适应学习率：从AdaGrad到Adam</a></li><li><a href="/%E4%BB%8E%E5%9B%9E%E5%BD%92%E5%88%B0%E5%88%86%E7%B1%BB%EF%BC%9ASoftmax%E4%B8%8E%E4%BA%A4%E5%8F%89%E7%86%B5/" rel="bookmark" title="从回归到分类：Softmax与交叉熵">从回归到分类：Softmax与交叉熵</a></li><li><a href="/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/" rel="bookmark" title="批量归一化">批量归一化</a></li><li class="active"><a href="/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN/" rel="bookmark" title="卷积神经网络CNN">卷积神经网络CNN</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="漠溟绽灵Stinnc" data-src="/images/hdd.jpg"><p class="name" itemprop="name">漠溟绽灵Stinnc</p><div class="description" itemprop="description">嘿嘿嘿~~</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">13</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">5</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">53</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0aW5uY3NreQ==" title="https:&#x2F;&#x2F;github.com&#x2F;stinncsky"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTExMDY5NDgzNA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;110694834"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3440-%E9%87%8D%E6%96%B0%E5%AE%89%E6%8E%92%E4%BC%9A%E8%AE%AE%E5%BE%97%E5%88%B0%E6%9C%80%E5%A4%9A%E7%A9%BA%E4%BD%99%E6%97%B6%E9%97%B4-II/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1900-%E6%9C%80%E4%BD%B3%E8%BF%90%E5%8A%A8%E5%91%98%E7%9A%84%E6%AF%94%E6%8B%BC%E5%9B%9E%E5%90%88/" title="力扣每日一题：1900. 最佳运动员的比拼回合">力扣每日一题：1900. 最佳运动员的比拼回合</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3169-%E6%97%A0%E9%9C%80%E5%BC%80%E4%BC%9A%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%97%A5/" title="力扣每日一题：3169. 无需开会的工作日">力扣每日一题：3169. 无需开会的工作日</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BF%87%E6%8B%9F%E5%90%88%E3%80%81%E9%9E%8D%E7%82%B9%E4%B8%8E%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/" title="模型训练问题：过拟合、鞍点与优化算法">模型训练问题：过拟合、鞍点与优化算法</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3439-%E9%87%8D%E6%96%B0%E5%AE%89%E6%8E%92%E4%BC%9A%E8%AE%AE%E5%BE%97%E5%88%B0%E6%9C%80%E5%A4%9A%E7%A9%BA%E4%BD%99%E6%97%B6%E9%97%B4-I/" title="力扣每日一题：3439. 重新安排会议得到最多空余时间 I">力扣每日一题：3439. 重新安排会议得到最多空余时间 I</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A2410-%E8%BF%90%E5%8A%A8%E5%91%98%E5%92%8C%E8%AE%AD%E7%BB%83%E5%B8%88%E7%9A%84%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%95%B0/" title="力扣每日一题：2410. 运动员和训练师的最大匹配数">力扣每日一题：2410. 运动员和训练师的最大匹配数</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%9A%E4%BB%8EAdaGrad%E5%88%B0Adam/" title="自适应学习率：从AdaGrad到Adam">自适应学习率：从AdaGrad到Adam</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1353-%E6%9C%80%E5%A4%9A%E5%8F%AF%E4%BB%A5%E5%8F%82%E5%8A%A0%E7%9A%84%E4%BC%9A%E8%AE%AE%E6%95%B0%E7%9B%AE/" title="力扣每日一题：1353. 最多可以参加的会议数目">力扣每日一题：1353. 最多可以参加的会议数目</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/" title="批量归一化">批量归一化</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%88%9D%E8%AF%86/" title="机器学习初识">机器学习初识</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/%E7%AE%97%E6%B3%95%E9%A2%98%E8%A7%A3/%E5%8A%9B%E6%89%A3/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" title="分类于 每日一题">每日一题</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1751-%E6%9C%80%E5%A4%9A%E5%8F%AF%E4%BB%A5%E5%8F%82%E5%8A%A0%E7%9A%84%E4%BC%9A%E8%AE%AE%E6%95%B0%E7%9B%AE-II/" title="力扣每日一题：1751. 最多可以参加的会议数目 II">力扣每日一题：1751. 最多可以参加的会议数目 II</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">漠溟绽灵Stinnc @ Stinnc</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">33k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">30 分钟</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"卷积神经网络CNN/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/js/index.js"></script></body></html>