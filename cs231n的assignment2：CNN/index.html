<!DOCTYPE html><html lang="zh-CN"><head><link href="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/css/index.css" rel="stylesheet"><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="漠溟绽灵的小站" href="https://stinncsky.github.io/atom.xml"><link rel="alternate" type="application/json" title="漠溟绽灵的小站" href="https://stinncsky.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="cs231n,反向传播,CNN,卷积神经网络,池化,批标准化,组归一化"><link rel="canonical" href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/"><title>cs231n的assignment2：CNN - assignment2 - cs231n - 代码实现 - 机器学习 - 学习笔记 | Stinnc = 漠溟绽灵的小站 = 谁知道里面有什么</title><meta name="generator" content="Hexo 7.3.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">cs231n的assignment2：CNN</h1><div class="meta"><span class="item" title="创建时间：2025-08-03 04:06:20"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">发表于</span> <time itemprop="dateCreated datePublished" datetime="2025-08-03T04:06:20+08:00">2025-08-03</time> </span><span class="item" title="本文字数"><span class="icon"><i class="ic i-pen"></i> </span><span class="text">本文字数</span> <span>23k</span> <span class="text">字</span> </span><span class="item" title="阅读时长"><span class="icon"><i class="ic i-clock"></i> </span><span class="text">阅读时长</span> <span>21 分钟</span></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="切换导航栏"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">Stinnc</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="/images/bd33d1b08260ad781843ed6667db3356ca53e6c5.jpg"></li><li class="item" data-background-image="/images/132363104_p0_master1200.jpg"></li><li class="item" data-background-image="/images/132319858_p0_master1200.jpg"></li><li class="item" data-background-image="/images/132357959_p0_master1200.jpg"></li><li class="item" data-background-image="/images/0ea650dd052f04785575d2e5feb6e9ea17c3d01f.jpg@1192w.avif"></li><li class="item" data-background-image="/images/132325152_p0_master1200.jpg"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">首页</a></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/" itemprop="item" rel="index" title="分类于 学习笔记"><span itemprop="name">学习笔记</span></a><meta itemprop="position" content="1"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/" itemprop="item" rel="index" title="分类于 机器学习"><span itemprop="name">机器学习</span></a><meta itemprop="position" content="2"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/" itemprop="item" rel="index" title="分类于 代码实现"><span itemprop="name">代码实现</span></a><meta itemprop="position" content="3"></span><i class="ic i-angle-right"></i> <span itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/cs231n/" itemprop="item" rel="index" title="分类于 cs231n"><span itemprop="name">cs231n</span></a><meta itemprop="position" content="4"></span><i class="ic i-angle-right"></i> <span class="current" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a href="/categories/study-notes/machine-learning/code-implementation/cs231n/assignment2/" itemprop="item" rel="index" title="分类于 assignment2"><span itemprop="name">assignment2</span></a><meta itemprop="position" content="5"></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/hdd.jpg"><meta itemprop="name" content="漠溟绽灵Stinnc"><meta itemprop="description" content="谁知道里面有什么, 嘿嘿嘿~~"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="漠溟绽灵的小站"></span><div class="body md" itemprop="articleBody"><h1 id="卷积神经网络前向传播"><a class="anchor" href="#卷积神经网络前向传播">#</a> 卷积神经网络前向传播</h1><p>卷积神经网络是深度学习中最重要的架构之一，其核心在于卷积操作能够有效提取图像的空间特征。</p><h2 id="卷积层前向传播原理"><a class="anchor" href="#卷积层前向传播原理">#</a> 卷积层前向传播原理</h2><p>卷积操作通过在输入特征图上滑动卷积核来提取局部特征。对于输入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">x</span></span></span></span> 和卷积核权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span>，卷积操作的数学表达为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>out</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow></munder><msub><mi>x</mi><mtext>padded</mtext></msub><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mi>i</mi><mo separator="true">,</mo><mi>w</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mi>j</mi><mo stretchy="false">]</mo><mo>⋅</mo><mi>w</mi><mo stretchy="false">[</mo><mi>f</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo><mo>+</mo><mi>b</mi><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{out}[n, f, h, w] = \sum_{c,i,j} x_{\text{padded}}[n, c, h \cdot \text{stride} + i, w \cdot \text{stride} + j] \cdot w[f, c, i, j] + b[f]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">out</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.463782em;vertical-align:-1.413777em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8723309999999997em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.05724em">j</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.413777em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">padded</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">b</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mclose">]</span></span></span></span></span></p><p>其中：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">n</span></span></span></span> 表示批次索引</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span></span></span></span> 表示输出通道（滤波器）索引</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo separator="true">,</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">h, w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span></span></span></span> 表示输出空间位置</li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><annotation encoding="application/x-tex">c, i, j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span></span></span></span> 表示输入通道和卷积核空间位置</li></ul><figure class="highlight python"><figcaption><span>卷积前向传播实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv_forward_naive</span>(<span class="params">x, w, b, conv_param</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - &#x27;pad&#x27;: The number of pixels that will be used to zero-pad the input.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, &#x27;pad&#x27; zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional forward pass.                         #</span></span><br><span class="line">    <span class="comment"># Hint: you can use the function np.pad for padding.                      #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    H_padded = H + <span class="number">2</span> * pad</span><br><span class="line">    W_padded = W + <span class="number">2</span> * pad</span><br><span class="line">    H_out = <span class="number">1</span> + (H_padded - HH) // stride</span><br><span class="line">    W_out = <span class="number">1</span> + (W_padded - WW) // stride</span><br><span class="line">    out = np.zeros((N, F, H_out, W_out))</span><br><span class="line">    x_padded = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(F):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_idx <span class="keyword">in</span> <span class="built_in">range</span>(W_out):</span><br><span class="line">                    h_start = h * stride</span><br><span class="line">                    w_start = w_idx * stride</span><br><span class="line">                    x_slice = x_padded[n, :, h_start:h_start + HH, w_start:w_start + WW]</span><br><span class="line">                    out[n, f, h, w_idx] = np.<span class="built_in">sum</span>(x_slice * w[f]) + b[f]</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><details class="info"><summary>输出尺寸计算</summary><div><p>输出特征图的尺寸遵循公式：</p><ul><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>H</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>H</mi><mo>+</mo><mn>2</mn><mo>×</mo><mtext>pad</mtext><mo>−</mo><mi>H</mi><mi>H</mi></mrow><mtext>stride</mtext></mfrac><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">H&#x27; = \frac{H + 2 \times \text{pad} - HH}{\text{stride}} + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.277216em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.9322159999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">stride</span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.446108em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.08125em">H</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight">pad</span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:.08125em">H</span><span class="mord mathnormal mtight" style="margin-right:.08125em">H</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span></li><li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo>=</mo><mfrac><mrow><mi>W</mi><mo>+</mo><mn>2</mn><mo>×</mo><mtext>pad</mtext><mo>−</mo><mi>W</mi><mi>W</mi></mrow><mtext>stride</mtext></mfrac><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">W&#x27; = \frac{W + 2 \times \text{pad} - WW}{\text{stride}} + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.751892em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.751892em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.277216em;vertical-align:-.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.9322159999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">stride</span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.446108em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">W</span><span class="mbin mtight">+</span><span class="mord mtight">2</span><span class="mbin mtight">×</span><span class="mord text mtight"><span class="mord mtight">pad</span></span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:.13889em">W</span><span class="mord mathnormal mtight" style="margin-right:.13889em">W</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">1</span></span></span></span></li></ul><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">HH, WW</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8777699999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mord mathnormal" style="margin-right:.13889em">W</span></span></span></span> 是卷积核的高度和宽度。填充（padding）用于控制输出尺寸，步长（stride）控制下采样程度。</p></div></details><div class="note info no-icon"><p>卷积操作的核心是感受野概念：每个输出元素对应输入中的一个局部区域，通过权重共享实现参数效率。</p></div><h1 id="卷积神经网络反向传播"><a class="anchor" href="#卷积神经网络反向传播">#</a> 卷积神经网络反向传播</h1><p>反向传播是训练卷积神经网络的关键步骤，通过链式法则计算各参数的梯度。</p><h2 id="卷积层反向传播原理"><a class="anchor" href="#卷积层反向传播原理">#</a> 卷积层反向传播原理</h2><p>根据链式法则，卷积层需要计算三个梯度：</p><h3 id="偏置梯度计算"><a class="anchor" href="#偏置梯度计算">#</a> 偏置梯度计算</h3><p>偏置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">b[f]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">b</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mclose">]</span></span></span></span> 影响该滤波器产生的所有输出位置，因此：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>b</mi><mo stretchy="false">[</mo><mi>f</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mrow><mi>n</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi></mrow></munder><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mtext>out</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mrow><mi>n</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi></mrow></munder><mtext>dout</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial b[f]} = \sum_{n,h,w} \frac{\partial L}{\partial \text{out}[n, f, h, w]} = \sum_{n,h,w} \text{dout}[n, f, h, w]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">b</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mclose">]</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.809661em;vertical-align:-1.438221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8478869999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.438221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord text"><span class="mord">out</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.488226em;vertical-align:-1.438221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8478869999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.438221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord text"><span class="mord">dout</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span></span></span></span></span></p><h3 id="权重梯度计算"><a class="anchor" href="#权重梯度计算">#</a> 权重梯度计算</h3><p>对于权重 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo stretchy="false">[</mo><mi>f</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">w[f, c, i, j]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span></span></span></span>：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>w</mi><mo stretchy="false">[</mo><mi>f</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mrow><mi>n</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi></mrow></munder><mtext>dout</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo><mo>×</mo><msub><mi>x</mi><mtext>padded</mtext></msub><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mi>i</mi><mo separator="true">,</mo><mi>w</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial w[f, c, i, j]} = \sum_{n,h,w} \text{dout}[n, f, h, w] \times x_{\text{padded}}[n, c, h \cdot \text{stride} + i, w \cdot \text{stride} + j]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.488226em;vertical-align:-1.438221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8478869999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.438221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord text"><span class="mord">dout</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.3361079999999999em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">padded</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.85396em;vertical-align:-.19444em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span></span></span></span></span></p><h3 id="输入梯度计算"><a class="anchor" href="#输入梯度计算">#</a> 输入梯度计算</h3><p>输入梯度是最复杂的，因为每个输入位置可能参与多个输出的计算：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>=</mo><munder><mo>∑</mo><mrow><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi></mrow></munder><mtext>dout</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>f</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo><mo>×</mo><mi>w</mi><mo stretchy="false">[</mo><mi>f</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>i</mi><mo>−</mo><mi>h</mi><mo>⋅</mo><mtext>stride</mtext><mo separator="true">,</mo><mi>j</mi><mo>−</mo><mi>w</mi><mo>⋅</mo><mtext>stride</mtext><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial x[n, c, i, j]} = \sum_{f,h,w} \text{dout}[n, f, h, w] \times w[f, c, i - h \cdot \text{stride}, j - w \cdot \text{stride}]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">x</span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.488226em;vertical-align:-1.438221em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em"><span style="top:-1.8478869999999998em;margin-left:0"><span class="pstrut" style="height:3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:.10764em">f</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span style="top:-3.0500049999999996em"><span class="pstrut" style="height:3.05em"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.438221em"><span></span></span></span></span></span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord text"><span class="mord">dout</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:.10764em">f</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.44445em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mclose">]</span></span></span></span></span></p><figure class="highlight python"><figcaption><span>卷积反向传播实现</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">conv_backward_naive</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the convolutional backward pass.                        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># 解包缓存</span></span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    F, _, HH, WW = w.shape</span><br><span class="line">    _, _, H_out, W_out = dout.shape</span><br><span class="line">    stride = conv_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    pad = conv_param[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化梯度</span></span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.zeros_like(b)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入进行填充（与前向传播保持一致）</span></span><br><span class="line">    x_padded = np.pad(x, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad)), </span><br><span class="line">                      mode=<span class="string">&#x27;constant&#x27;</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    dx_padded = np.zeros_like(x_padded)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(F):</span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(H_out):</span><br><span class="line">                <span class="keyword">for</span> w_idx <span class="keyword">in</span> <span class="built_in">range</span>(W_out):</span><br><span class="line">                    h_start = h * stride</span><br><span class="line">                    w_start = w_idx * stride</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 使用填充后的输入提取感受野</span></span><br><span class="line">                    x_slice = x_padded[n, :, h_start:h_start + HH, w_start:w_start + WW]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># 计算梯度</span></span><br><span class="line">                    db[f] += dout[n, f, h, w_idx]</span><br><span class="line">                    dw[f] += x_slice * dout[n, f, h, w_idx]</span><br><span class="line">                    dx_padded[n, :, h_start:h_start + HH, w_start:w_start + WW] += \</span><br><span class="line">                        dout[n, f, h, w_idx] * w[f]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 去除填充，恢复原始输入的梯度</span></span><br><span class="line">    <span class="keyword">if</span> pad &gt; <span class="number">0</span>:</span><br><span class="line">        dx = dx_padded[:, :, pad:-pad, pad:-pad]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        dx = dx_padded</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure><div class="note info no-icon"><p>反向传播的核心原则：梯度沿着前向传播的计算图反向流动，在每个节点根据局部导数进行分发和累积。</p></div><details class="info"><summary>梯度累积机制</summary><div><p>在卷积反向传播中，梯度累积体现了参数重用的特性：</p><ul><li><strong>偏置梯度</strong>：一个偏置影响该滤波器的所有输出，需累积所有位置的梯度</li><li><strong>权重梯度</strong>：一个权重在不同位置重复使用，需累积所有使用位置的贡献</li><li><strong>输入梯度</strong>：一个输入可能被多个输出的感受野包含，需累积所有相关输出的反向传播</li></ul></div></details><h1 id="池化层实现"><a class="anchor" href="#池化层实现">#</a> 池化层实现</h1><p>池化层通过下采样减少特征图尺寸，提高计算效率并增强平移不变性。</p><h2 id="最大池化前向传播"><a class="anchor" href="#最大池化前向传播">#</a> 最大池化前向传播</h2><p>最大池化选择每个池化窗口内的最大值作为输出：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>out</mtext><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">[</mo><mi>n</mi><mo separator="true">,</mo><mi>c</mi><mo separator="true">,</mo><mi>h</mi><mo>⋅</mo><mtext>stride</mtext><mo>:</mo><mo stretchy="false">(</mo><mi>h</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mtext>pool_height</mtext><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>w</mi><mo>⋅</mo><mtext>stride</mtext><mo>:</mo><mo stretchy="false">(</mo><mi>w</mi><mo>⋅</mo><mtext>stride</mtext><mo>+</mo><mtext>pool_width</mtext><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{out}[n, c, h, w] = \max(x[n, c, h \cdot \text{stride}:(h \cdot \text{stride} + \text{pool\_height}), w \cdot \text{stride}:(w \cdot \text{stride} + \text{pool\_width})])</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord text"><span class="mord">out</span></span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mclose">]</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mopen">[</span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">c</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-.31em"></span><span class="mord text"><span class="mord">pool_height</span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">:</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.77777em;vertical-align:-.08333em"></span><span class="mord text"><span class="mord">stride</span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.06em;vertical-align:-.31em"></span><span class="mord text"><span class="mord">pool_width</span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mclose">)</span></span></span></span></span></p><figure class="highlight python"><figcaption><span>最大池化前向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pool_forward_naive</span>(<span class="params">x, pool_param</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - &#x27;pool_height&#x27;: The height of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;pool_width&#x27;: The width of each pooling region</span></span><br><span class="line"><span class="string">      - &#x27;stride&#x27;: The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here, eg you can assume:</span></span><br><span class="line"><span class="string">      - (H - pool_height) % stride == 0</span></span><br><span class="line"><span class="string">      - (W - pool_width) % stride == 0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H&#x27;, W&#x27;) where H&#x27; and W&#x27; are given by</span></span><br><span class="line"><span class="string">      H&#x27; = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W&#x27; = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max-pooling forward pass                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    H_out = <span class="number">1</span> + (H - pool_height) / stride</span><br><span class="line">    W_out = <span class="number">1</span> + (W - pool_width) / stride</span><br><span class="line">    out = np.zeros((N, C, <span class="built_in">int</span>(H_out), <span class="built_in">int</span>(W_out)))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(H_out)):</span><br><span class="line">                <span class="keyword">for</span> w_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(W_out)):</span><br><span class="line">                    h_start = h * stride</span><br><span class="line">                    w_start = w_idx * stride</span><br><span class="line">                    x_slice = x[n, c, h_start:h_start + pool_height, w_start:w_start + pool_width]</span><br><span class="line">                    out[n, c, h, w_idx] = np.<span class="built_in">max</span>(x_slice)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><h2 id="最大池化反向传播"><a class="anchor" href="#最大池化反向传播">#</a> 最大池化反向传播</h2><p>最大池化的反向传播只将梯度传递给最大值位置：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>x</mi><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow></mfrac><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mtext>out</mtext><mo stretchy="false">[</mo><mi>h</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">]</mo></mrow></mfrac></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>如果</mtext><mi>x</mi><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo><mtext>是池化区域的最大值</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>否则</mtext></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial x[i,j]} = \begin{cases} \frac{\partial L}{\partial \text{out}[h,w]} &amp; \text{如果 } x[i,j] \text{ 是池化区域的最大值} \\ 0 &amp; \text{否则} \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.30744em;vertical-align:-.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.37144em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">x</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord" style="margin-right:.05556em">∂</span><span class="mord mathnormal">L</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em"></span><span class="minner"><span class="mopen delimcenter" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.734em"><span style="top:-3.734em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.8801079999999999em"><span style="top:-2.655em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:.05556em">∂</span><span class="mord text mtight"><span class="mord mtight">out</span></span><span class="mopen mtight">[</span><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span><span class="mclose mtight">]</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:.05556em">∂</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.52em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span style="top:-2.206em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.234em"><span></span></span></span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.734em"><span style="top:-3.734em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">如果</span><span class="mord"> </span></span><span class="mord mathnormal">x</span><span class="mopen">[</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:.05724em">j</span><span class="mclose">]</span><span class="mord text"><span class="mord"> </span><span class="mord cjk_fallback">是池化区域的最大值</span></span></span></span><span style="top:-2.206em"><span class="pstrut" style="height:3.008em"></span><span class="mord"><span class="mord text"><span class="mord cjk_fallback">否则</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.234em"><span></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><figure class="highlight python"><figcaption><span>最大池化反向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">max_pool_backward_naive</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the max-pooling backward pass                           #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    pool_height = pool_param[<span class="string">&#x27;pool_height&#x27;</span>]</span><br><span class="line">    pool_width = pool_param[<span class="string">&#x27;pool_width&#x27;</span>]</span><br><span class="line">    stride = pool_param[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    dx = np.zeros_like(x)  <span class="comment"># 初始化输入梯度</span></span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(N):</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(C):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, H, stride):</span><br><span class="line">                <span class="keyword">for</span> w_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, W, stride):</span><br><span class="line">                    h_start = h</span><br><span class="line">                    w_start = w_idx</span><br><span class="line">                    x_slice = x[n, c, h_start:h_start + pool_height, w_start:w_start + pool_width]</span><br><span class="line">                    max_val = np.<span class="built_in">max</span>(x_slice)</span><br><span class="line">                    mask = (x_slice == max_val)  <span class="comment"># 布尔掩码，最大值位置为True</span></span><br><span class="line">                    dx[n, c, h_start:h_start + pool_height, w_start:w_start + pool_width] += \</span><br><span class="line">                        mask * dout[n, c, h // stride, w_idx // stride]</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure><details class="info"><summary>池化反向传播的掩码机制</summary><div><p>最大池化使用布尔掩码来标识最大值位置：</p><ul><li>创建掩码： <code>mask = (x_slice == max_val)</code></li><li>梯度分配：只有最大值位置接收上游梯度</li><li>处理重复最大值：通常所有最大值位置都接收梯度</li></ul></div></details><div class="note info no-icon"><p>池化操作是<strong>不可学习</strong>的：它不包含可训练参数，仅通过固定的选择策略进行特征下采样。</p></div><h1 id="卷积神经网络架构设计"><a class="anchor" href="#卷积神经网络架构设计">#</a> 卷积神经网络架构设计</h1><p>现代卷积神经网络通过组合多种层类型构建深度架构，实现端到端的特征学习。</p><h2 id="三层卷积网络实现"><a class="anchor" href="#三层卷积网络实现">#</a> 三层卷积网络实现</h2><p>典型的三层卷积网络架构为： <code>conv - relu - 2x2 max pool - affine - relu - affine - softmax</code></p><figure class="highlight python"><figcaption><span>三层卷积网络架构</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ThreeLayerConvNet</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A three-layer convolutional network with the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    conv - relu - 2x2 max pool - affine - relu - affine - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The network operates on minibatches of data that have shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    consisting of N images, each with height H and width W and with C input</span></span><br><span class="line"><span class="string">    channels.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        input_dim=(<span class="params"><span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span></span>),</span></span><br><span class="line"><span class="params">        num_filters=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">        filter_size=<span class="number">7</span>,</span></span><br><span class="line"><span class="params">        hidden_dim=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">        num_classes=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">        weight_scale=<span class="number">1e-3</span>,</span></span><br><span class="line"><span class="params">        reg=<span class="number">0.0</span>,</span></span><br><span class="line"><span class="params">        dtype=np.float32,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Initialize a new network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_dim: Tuple (C, H, W) giving size of input data</span></span><br><span class="line"><span class="string">        - num_filters: Number of filters to use in the convolutional layer</span></span><br><span class="line"><span class="string">        - filter_size: Width/height of filters to use in the convolutional layer</span></span><br><span class="line"><span class="string">        - hidden_dim: Number of units to use in the fully-connected hidden layer</span></span><br><span class="line"><span class="string">        - num_classes: Number of scores to produce from the final affine layer.</span></span><br><span class="line"><span class="string">        - weight_scale: Scalar giving standard deviation for random initialization</span></span><br><span class="line"><span class="string">          of weights.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving L2 regularization strength</span></span><br><span class="line"><span class="string">        - dtype: numpy datatype to use for computation.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.params = &#123;&#125;</span><br><span class="line">        <span class="variable language_">self</span>.reg = reg</span><br><span class="line">        <span class="variable language_">self</span>.dtype = dtype</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Initialize weights and biases for the three-layer convolutional    #</span></span><br><span class="line">        <span class="comment"># network. Weights should be initialized from a Gaussian centered at 0.0   #</span></span><br><span class="line">        <span class="comment"># with standard deviation equal to weight_scale; biases should be          #</span></span><br><span class="line">        <span class="comment"># initialized to zero. All weights and biases should be stored in the      #</span></span><br><span class="line">        <span class="comment">#  dictionary self.params. Store weights and biases for the convolutional  #</span></span><br><span class="line">        <span class="comment"># layer using the keys &#x27;W1&#x27; and &#x27;b1&#x27;; use keys &#x27;W2&#x27; and &#x27;b2&#x27; for the       #</span></span><br><span class="line">        <span class="comment"># weights and biases of the hidden affine layer, and keys &#x27;W3&#x27; and &#x27;b3&#x27;    #</span></span><br><span class="line">        <span class="comment"># for the weights and biases of the output affine layer.                   #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># IMPORTANT: For this assignment, you can assume that the padding          #</span></span><br><span class="line">        <span class="comment"># and stride of the first convolutional layer are chosen so that           #</span></span><br><span class="line">        <span class="comment"># **the width and height of the input are preserved**. Take a look at      #</span></span><br><span class="line">        <span class="comment"># the start of the loss() function to see how that happens.                #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;W1&quot;</span>] = np.random.normal(</span><br><span class="line">            <span class="number">0</span>, weight_scale, (num_filters, input_dim[<span class="number">0</span>], filter_size, filter_size)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;b1&quot;</span>] = np.zeros(num_filters)</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;W2&quot;</span>] = np.random.normal(</span><br><span class="line">            <span class="number">0</span>, weight_scale, (num_filters * (input_dim[<span class="number">1</span>] // <span class="number">2</span>) * (input_dim[<span class="number">2</span>] // <span class="number">2</span>), hidden_dim)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;b2&quot;</span>] = np.zeros(hidden_dim)</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;W3&quot;</span>] = np.random.normal(</span><br><span class="line">            <span class="number">0</span>, weight_scale, (hidden_dim, num_classes)</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.params[<span class="string">&quot;b3&quot;</span>] = np.zeros(num_classes)</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="variable language_">self</span>.params.items():</span><br><span class="line">            <span class="variable language_">self</span>.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Evaluate loss and gradient for the three-layer convolutional network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / output: Same API as TwoLayerNet in fc_net.py.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        W1, b1 = <span class="variable language_">self</span>.params[<span class="string">&quot;W1&quot;</span>], <span class="variable language_">self</span>.params[<span class="string">&quot;b1&quot;</span>]</span><br><span class="line">        W2, b2 = <span class="variable language_">self</span>.params[<span class="string">&quot;W2&quot;</span>], <span class="variable language_">self</span>.params[<span class="string">&quot;b2&quot;</span>]</span><br><span class="line">        W3, b3 = <span class="variable language_">self</span>.params[<span class="string">&quot;W3&quot;</span>], <span class="variable language_">self</span>.params[<span class="string">&quot;b3&quot;</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass conv_param to the forward pass for the convolutional layer</span></span><br><span class="line">        <span class="comment"># Padding and stride chosen to preserve the input spatial size</span></span><br><span class="line">        filter_size = W1.shape[<span class="number">2</span>]</span><br><span class="line">        conv_param = &#123;<span class="string">&quot;stride&quot;</span>: <span class="number">1</span>, <span class="string">&quot;pad&quot;</span>: (filter_size - <span class="number">1</span>) // <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pass pool_param to the forward pass for the max-pooling layer</span></span><br><span class="line">        pool_param = &#123;<span class="string">&quot;pool_height&quot;</span>: <span class="number">2</span>, <span class="string">&quot;pool_width&quot;</span>: <span class="number">2</span>, <span class="string">&quot;stride&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for the three-layer convolutional net,  #</span></span><br><span class="line">        <span class="comment"># computing the class scores for X and storing them in the scores          #</span></span><br><span class="line">        <span class="comment"># variable.                                                                #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># Remember you can use the functions defined in cs231n/fast_layers.py and  #</span></span><br><span class="line">        <span class="comment"># cs231n/layer_utils.py in your implementation (already imported).         #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># </span></span><br><span class="line">        out1, cache1 = conv_relu_pool_forward(X, W1, b1, conv_param, pool_param)</span><br><span class="line">        out2, cache2 = affine_relu_forward(out1, W2, b2)</span><br><span class="line">        scores, cache3 = affine_forward(out2, W3, b3)</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0</span>, &#123;&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for the three-layer convolutional net, #</span></span><br><span class="line">        <span class="comment"># storing the loss and gradients in the loss and grads variables. Compute  #</span></span><br><span class="line">        <span class="comment"># data loss using softmax, and make sure that grads[k] holds the gradients #</span></span><br><span class="line">        <span class="comment"># for self.params[k]. Don&#x27;t forget to add L2 regularization!               #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> To ensure that your implementation matches ours and you pass the   #</span></span><br><span class="line">        <span class="comment"># automated tests, make sure that your L2 regularization includes a factor #</span></span><br><span class="line">        <span class="comment"># of 0.5 to simplify the expression for the gradient.                      #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        loss, dscores = softmax_loss(scores, y)</span><br><span class="line">        <span class="comment"># Add L2 regularization to the loss</span></span><br><span class="line">        loss += <span class="number">0.5</span> * <span class="variable language_">self</span>.reg * (np.<span class="built_in">sum</span>(W1 ** <span class="number">2</span>) + np.<span class="built_in">sum</span>(W2 ** <span class="number">2</span>) + np.<span class="built_in">sum</span>(W3 ** <span class="number">2</span>))</span><br><span class="line">        dx3, grads[<span class="string">&quot;W3&quot;</span>], grads[<span class="string">&quot;b3&quot;</span>] = affine_backward(dscores, cache3)</span><br><span class="line">        grads[<span class="string">&quot;W3&quot;</span>] += <span class="variable language_">self</span>.reg * W3  <span class="comment"># Add L2 regularization</span></span><br><span class="line">        dx2, grads[<span class="string">&quot;W2&quot;</span>], grads[<span class="string">&quot;b2&quot;</span>] = affine_relu_backward(dx3, cache2)</span><br><span class="line">        grads[<span class="string">&quot;W2&quot;</span>] += <span class="variable language_">self</span>.reg * W2  <span class="comment"># Add L2 regularization</span></span><br><span class="line">        dx1, grads[<span class="string">&quot;W1&quot;</span>], grads[<span class="string">&quot;b1&quot;</span>] = conv_relu_pool_backward(dx2, cache1)</span><br><span class="line">        grads[<span class="string">&quot;W1&quot;</span>] += <span class="variable language_">self</span>.reg * W1  <span class="comment"># Add L2 regularization</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure><details class="info"><summary>架构设计要点</summary><div><p>三层卷积网络的关键设计决策：</p><ul><li><strong>填充策略</strong>: <code>pad = (filter_size - 1) // 2</code> 保持空间尺寸不变</li><li><strong>池化下采样</strong>: 2×2 最大池化将特征图尺寸减半</li><li><strong>维度转换</strong>：卷积输出需要展平后连接到全连接层</li><li><strong>正则化</strong>: L2 正则化防止过拟合，系数 0.5 简化梯度计算</li></ul></div></details><div class="note info no-icon"><p>现代 CNN 架构的核心思想：通过<strong>局部连接</strong>和<strong>权重共享</strong>实现参数效率，通过<strong>分层特征提取</strong>实现表示学习。</p></div><h1 id="批标准化技术"><a class="anchor" href="#批标准化技术">#</a> 批标准化技术</h1><p>批标准化是加速神经网络训练的重要技术，通过标准化激活值分布来稳定训练过程。</p><h2 id="空间批标准化"><a class="anchor" href="#空间批标准化">#</a> 空间批标准化</h2><p>空间批标准化专门针对卷积层设计，对每个通道独立进行标准化：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi>μ</mi></mrow><msqrt><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-.22222em"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.1903300000000003em;vertical-align:-.93em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.2603300000000002em"><span style="top:-2.196611em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.913389em"><span class="svg-align" style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord" style="padding-left:.833em"><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.740108em"><span style="top:-2.9890000000000003em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.873389em"><span class="pstrut" style="height:3em"></span><span class="hide-tail" style="min-width:.853em;height:1.08em"><svg width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.12661100000000003em"><span></span></span></span></span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mord mathnormal">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.93em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p><p>其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>μ</mi></mrow><annotation encoding="application/x-tex">\mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.625em;vertical-align:-.19444em"></span><span class="mord mathnormal">μ</span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8141079999999999em;vertical-align:0"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03588em">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8141079999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 分别是跨批次和空间维度计算的均值和方差。</p><figure class="highlight python"><figcaption><span>空间批标准化前向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">spatial_batchnorm_forward</span>(<span class="params">x, gamma, beta, bn_param</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the forward pass for spatial batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - beta: Shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: &#x27;train&#x27; or &#x27;test&#x27;; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance. momentum=0 means that</span></span><br><span class="line"><span class="string">        old information is discarded completely at every time step, while</span></span><br><span class="line"><span class="string">        momentum=1 means that new information is never incorporated. The</span></span><br><span class="line"><span class="string">        default of momentum=0.9 should work well in most situations.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for spatial batch normalization.       #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You can implement spatial batch normalization by calling the      #</span></span><br><span class="line">    <span class="comment"># vanilla version of batch normalization you implemented above.           #</span></span><br><span class="line">    <span class="comment"># Your implementation should be very short; ours is less than five lines. #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_reshaped = x.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(-<span class="number">1</span>, C)  <span class="comment"># Reshape to (N*H*W, C)</span></span><br><span class="line">    out_reshaped, cache = batchnorm_forward(x_reshaped, gamma, beta, bn_param)</span><br><span class="line">    out = out_reshaped.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># Reshape back to (N, C, H, W)</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>空间批标准化反向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">spatial_batchnorm_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the backward pass for spatial batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter, of shape (C,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for spatial batch normalization.      #</span></span><br><span class="line">    <span class="comment">#                                                                         #</span></span><br><span class="line">    <span class="comment"># HINT: You can implement spatial batch normalization by calling the      #</span></span><br><span class="line">    <span class="comment"># vanilla version of batch normalization you implemented above.           #</span></span><br><span class="line">    <span class="comment"># Your implementation should be very short; ours is less than five lines. #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = dout.shape</span><br><span class="line">    dout_reshaped = dout.transpose(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>).reshape(-<span class="number">1</span>, C)  <span class="comment"># Reshape to (N*H*W, C)</span></span><br><span class="line">    dx_reshaped, dgamma, dbeta = batchnorm_backward(dout_reshaped, cache)</span><br><span class="line">    dx = dx_reshaped.reshape(N, H, W, C).transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># Reshape back to (N, C, H, W)</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><table><thead><tr><th><strong>特性</strong></th><th>标准批标准化</th><th>空间批标准化</th></tr></thead><tbody><tr><td><strong>输入形状</strong></td><td><code>(N, D)</code></td><td><code>(N, C, H, W)</code></td></tr><tr><td><strong>归一化维度</strong></td><td>仅 <code>N</code> (小批量)</td><td><code>N + H + W</code> (跨样本和空间)</td></tr><tr><td><strong>统计量形状</strong></td><td><code>(D,)</code></td><td><code>(C,)</code></td></tr><tr><td><strong>适用层类型</strong></td><td>全连接层</td><td>卷积层</td></tr><tr><td><strong>计算复杂度</strong></td><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(ND)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mord mathnormal" style="margin-right:.02778em">D</span><span class="mclose">)</span></span></span></span></td><td><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>N</mi><mi>C</mi><mi>H</mi><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(NCHW)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal" style="margin-right:.02778em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.10903em">N</span><span class="mord mathnormal" style="margin-right:.07153em">C</span><span class="mord mathnormal" style="margin-right:.08125em">H</span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mclose">)</span></span></span></span></td></tr></tbody></table><details class="info"><summary>维度变换技巧</summary><div><p>空间批标准化通过巧妙的维度变换复用标准批标准化：</p><ol><li>输入重塑： <code>(N, C, H, W) → (N*H*W, C)</code></li><li>应用标准批标准化处理每个通道</li><li>输出重塑： <code>(N*H*W, C) → (N, C, H, W)</code></li></ol><p>这种方法确保每个通道的所有空间位置使用相同的标准化参数。</p></div></details><div class="note info no-icon"><p>批标准化的核心作用：<strong>减少内部协变量偏移</strong>，使网络对参数初始化更加鲁棒，允许使用更大的学习率。</p></div><h1 id="组归一化技术"><a class="anchor" href="#组归一化技术">#</a> 组归一化技术</h1><p>组归一化是批标准化的改进版本，解决了小批量情况下批标准化性能下降的问题。</p><h2 id="组归一化原理"><a class="anchor" href="#组归一化原理">#</a> 组归一化原理</h2><p>组归一化将通道分组，在每组内进行归一化：</p><table><thead><tr><th>归一化类型</th><th>归一化维度</th><th>特点</th></tr></thead><tbody><tr><td><strong>Batch Norm</strong></td><td>跨样本 (N,H,W)</td><td>依赖批次大小</td></tr><tr><td><strong>Layer Norm</strong></td><td>跨通道 (C,H,W)</td><td>每个样本独立</td></tr><tr><td><strong>Group Norm</strong></td><td>跨组内通道</td><td>介于 BN 和 LN 之间</td></tr></tbody></table><figure class="highlight python"><figcaption><span>组归一化前向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">spatial_groupnorm_forward</span>(<span class="params">x, gamma, beta, G, gn_param</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the forward pass for spatial group normalization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    In contrast to layer normalization, group normalization splits each entry in the data into G</span></span><br><span class="line"><span class="string">    contiguous pieces, which it then normalizes independently. Per-feature shifting and scaling</span></span><br><span class="line"><span class="string">    are then applied to the data, in a manner identical to that of batch normalization and layer</span></span><br><span class="line"><span class="string">    normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter, of shape (1, C, 1, 1)</span></span><br><span class="line"><span class="string">    - beta: Shift parameter, of shape (1, C, 1, 1)</span></span><br><span class="line"><span class="string">    - G: Integer mumber of groups to split into, should be a divisor of C</span></span><br><span class="line"><span class="string">    - gn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values needed for the backward pass</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    eps = gn_param.get(<span class="string">&quot;eps&quot;</span>, <span class="number">1e-5</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for spatial group normalization.       #</span></span><br><span class="line">    <span class="comment"># This will be extremely similar to the layer norm implementation.        #</span></span><br><span class="line">    <span class="comment"># In particular, think about how you could transform the matrix so that   #</span></span><br><span class="line">    <span class="comment"># the bulk of the code is similar to both train-time batch normalization  #</span></span><br><span class="line">    <span class="comment"># and layer normalization!                                                #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    x_reshaped = x.reshape(N, G, C // G, H, W)  <span class="comment"># Shape: (N, G, C//G, H, W)</span></span><br><span class="line">    </span><br><span class="line">    x_mean = np.mean(x_reshaped, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)  <span class="comment"># Mean over (C//G, H, W)</span></span><br><span class="line">    x_var = np.var(x_reshaped, axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)  <span class="comment"># Variance over (C//G, H, W)</span></span><br><span class="line">    x_norm = (x_reshaped - x_mean) / np.sqrt(x_var + eps)  <span class="comment"># Normalize</span></span><br><span class="line">    x_norm = x_norm.reshape(N, C, H, W)  <span class="comment"># Reshape back to original shape</span></span><br><span class="line">    </span><br><span class="line">    out = gamma * x_norm + beta  <span class="comment"># Scale and shift</span></span><br><span class="line">    cache = (x, x_norm, x_mean, x_var, eps, gamma, beta, G)  <span class="comment"># Store values needed for backward pass</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><figcaption><span>组归一化反向传播</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">spatial_groupnorm_backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Computes the backward pass for spatial group normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - cache: Values from the forward pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter, of shape (1, C, 1, 1)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter, of shape (1, C, 1, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for spatial group normalization.      #</span></span><br><span class="line">    <span class="comment"># This will be extremely similar to the layer norm implementation.        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x, x_norm, x_mean, x_var, eps, gamma, beta, G = cache</span><br><span class="line">    N, C, H, W = x.shape</span><br><span class="line">    dgamma = np.<span class="built_in">sum</span>(dout * x_norm, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)  <span class="comment"># Gradient w.r.t. gamma</span></span><br><span class="line">    dbeta = np.<span class="built_in">sum</span>(dout, axis=(<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>), keepdims=<span class="literal">True</span>)  <span class="comment"># Gradient w.r.t. beta</span></span><br><span class="line">    dx_norm = dout * gamma  <span class="comment"># Gradient w.r.t. normalized input</span></span><br><span class="line">    dx_norm_reshaped = dx_norm.reshape(N, G, C // G, H, W)  <span class="comment"># Reshape to (N, G, C//G, H, W)</span></span><br><span class="line">    x_reshaped = x.reshape(N, G, C // G, H, W)  <span class="comment"># Reshape to (N, G, C//G, H, W)</span></span><br><span class="line">    D = C // G * H * W  <span class="comment"># Number of elements in each group</span></span><br><span class="line">    dx_var = np.<span class="built_in">sum</span>(dx_norm_reshaped * (x_reshaped - x_mean) * </span><br><span class="line">                    (-<span class="number">0.5</span>) * np.power(x_var + eps, -<span class="number">1.5</span>), </span><br><span class="line">                    axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    dx_mean = (np.<span class="built_in">sum</span>(dx_norm_reshaped * (-<span class="number">1.0</span>) / np.sqrt(x_var + eps), </span><br><span class="line">                     axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>) + </span><br><span class="line">              dx_var * np.<span class="built_in">sum</span>(<span class="number">2.0</span> * (x_reshaped - x_mean), </span><br><span class="line">                             axis=(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), keepdims=<span class="literal">True</span>) / D)</span><br><span class="line">    </span><br><span class="line">    dx_reshaped = (dx_norm_reshaped / np.sqrt(x_var + eps) + </span><br><span class="line">                   dx_var * <span class="number">2.0</span> * (x_reshaped - x_mean) / D + </span><br><span class="line">                   dx_mean / D)</span><br><span class="line">    </span><br><span class="line">    dx = dx_reshaped.reshape(N, C, H, W)  <span class="comment"># Reshape back to original shape</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure><details class="info"><summary>组归一化的优势</summary><div><p>组归一化相比批标准化的主要优势：</p><ul><li><strong>批量大小无关性</strong>：不依赖批次大小，适用于小批量训练</li><li><strong>计算效率</strong>：每个样本独立计算，便于并行化</li><li><strong>稳定性</strong>：避免了批标准化中不同批次间的统计量差异</li><li><strong>灵活性</strong>：通过调整组数 G，可以在批标准化和层标准化间插值</li></ul></div></details><div class="note info no-icon"><p>组归一化的核心思想：将通道分成 G 个组，每组内独立进行归一化，兼具批标准化的通道特异性和层标准化的批量无关性。</p></div><h1 id="卷积层优化技术"><a class="anchor" href="#卷积层优化技术">#</a> 卷积层优化技术</h1><p>朴素的卷积实现计算效率较低，现代深度学习框架采用多种优化技术加速卷积运算。</p><h2 id="im2col优化技术"><a class="anchor" href="#im2col优化技术">#</a> im2col 优化技术</h2><p>im2col 将卷积转换为矩阵乘法，这是最重要的优化手段：</p><details class="info"><summary>im2col变换原理</summary><div><p>im2col 将输入特征图的每个感受野展开为列向量：</p><ol><li><strong>输入变换</strong>: <code>(N, C, H, W) → (C×HH×WW, N×H_out×W_out)</code></li><li><strong>权重重塑</strong>: <code>(F, C, HH, WW) → (F, C×HH×WW)</code></li><li><strong>矩阵乘法</strong>: <code>(F, C×HH×WW) × (C×HH×WW, N×H_out×W_out) = (F, N×H_out×W_out)</code></li><li><strong>输出重塑</strong>: <code>(F, N×H_out×W_out) → (N, F, H_out, W_out)</code></li></ol><p>这种变换将空间卷积转化为高效的矩阵运算，充分利用 BLAS 库的优化。</p></div></details><h2 id="内存优化策略"><a class="anchor" href="#内存优化策略">#</a> 内存优化策略</h2><div class="tab" data-id="optimization" data-title="内存池化"><p>预分配固定大小的内存缓冲区，避免频繁的内存分配和释放：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ConvolutionLayer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_shape, num_filters, filter_size</span>):</span><br><span class="line">        <span class="comment"># 预分配内存缓冲区</span></span><br><span class="line">        <span class="variable language_">self</span>.x_cols_buffer = np.zeros((C * filter_size * filter_size, N * H_out * W_out))</span><br><span class="line">        <span class="variable language_">self</span>.output_buffer = np.zeros(<span class="variable language_">self</span>.output_shape)</span><br></pre></td></tr></table></figure></div><div class="tab" data-id="optimization" data-title="计算图优化"><p>通过融合相邻操作减少内存访问：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 融合 conv + relu + pool 为单一操作</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_relu_pool_forward</span>(<span class="params">x, w, b, conv_param, pool_param</span>):</span><br><span class="line">    conv_out, conv_cache = conv_forward_fast(x, w, b, conv_param)</span><br><span class="line">    relu_out, relu_cache = relu_forward(conv_out)</span><br><span class="line">    pool_out, pool_cache = max_pool_forward_fast(relu_out, pool_param)</span><br><span class="line">    cache = (conv_cache, relu_cache, pool_cache)</span><br><span class="line">    <span class="keyword">return</span> pool_out, cache</span><br></pre></td></tr></table></figure></div><div class="note info no-icon"><p>现代卷积优化的核心思路：通过<strong>算法变换</strong>和<strong>内存管理</strong>将空间操作转化为高效的线性代数运算。</p></div><div class="tags"><a href="/tags/cs231n/" rel="tag"><i class="ic i-tag"></i> cs231n</a> <a href="/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" rel="tag"><i class="ic i-tag"></i> 反向传播</a> <a href="/tags/CNN/" rel="tag"><i class="ic i-tag"></i> CNN</a> <a href="/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"><i class="ic i-tag"></i> 卷积神经网络</a> <a href="/tags/%E6%B1%A0%E5%8C%96/" rel="tag"><i class="ic i-tag"></i> 池化</a> <a href="/tags/%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96/" rel="tag"><i class="ic i-tag"></i> 批标准化</a> <a href="/tags/%E7%BB%84%E5%BD%92%E4%B8%80%E5%8C%96/" rel="tag"><i class="ic i-tag"></i> 组归一化</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">更新于</span> <time title="修改时间：2025-08-10 21:17:38" itemprop="dateModified" datetime="2025-08-10T21:17:38+08:00">2025-08-10</time> </span><span id="cs231n的assignment2：CNN/" class="item leancloud_visitors" data-flag-title="cs231n的assignment2：CNN" title="阅读次数"><span class="icon"><i class="ic i-eye"></i> </span><span class="text">阅读次数</span> <span class="leancloud-visitors-count"></span> <span class="text">次</span></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> 赞赏</button><p>请我喝[茶]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.jpg" alt="漠溟绽灵Stinnc 微信支付"><p>微信支付</p></div><div><img data-src="/images/alipay.jpg" alt="漠溟绽灵Stinnc 支付宝"><p>支付宝</p></div></div></div><div id="copyright"><ul><li class="author"><strong>本文作者： </strong>漠溟绽灵Stinnc <i class="ic i-at"><em>@</em></i>漠溟绽灵的小站</li><li class="link"><strong>本文链接：</strong> <a href="https://stinncsky.github.io/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/" title="cs231n的assignment2：CNN">https://stinncsky.github.io/cs231n的assignment2：CNN/</a></li><li class="license"><strong>版权声明： </strong>本站所有文章除特别声明外，均采用 <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> 许可协议。转载请注明出处！</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ADropout/" itemprop="url" rel="prev" data-background-image="&#x2F;images&#x2F;132338096_p0.png" title="cs231n的assignment2：Dropout"><span class="type">上一篇</span> <span class="category"><i class="ic i-flag"></i> assignment2</span><h3>cs231n的assignment2：Dropout</h3></a></div><div class="item right"><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A904-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE/" itemprop="url" rel="next" data-background-image="&#x2F;images&#x2F;bd33d1b08260ad781843ed6667db3356ca53e6c5.jpg" title="力扣每日一题：904. 水果成篮"><span class="type">下一篇</span> <span class="category"><i class="ic i-flag"></i> 每日一题</span><h3>力扣每日一题：904. 水果成篮</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="文章目录"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">1.</span> <span class="toc-text">卷积神经网络前向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.</span> <span class="toc-text">卷积层前向传播原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.</span> <span class="toc-text">卷积神经网络反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.</span> <span class="toc-text">卷积层反向传播原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E7%BD%AE%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">2.1.1.</span> <span class="toc-text">偏置梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">2.1.2.</span> <span class="toc-text">权重梯度计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97"><span class="toc-number">2.1.3.</span> <span class="toc-text">输入梯度计算</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.</span> <span class="toc-text">池化层实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.1.</span> <span class="toc-text">最大池化前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.2.</span> <span class="toc-text">最大池化反向传播</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.</span> <span class="toc-text">卷积神经网络架构设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%B1%82%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0"><span class="toc-number">4.1.</span> <span class="toc-text">三层卷积网络实现</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">5.</span> <span class="toc-text">批标准化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%BA%E9%97%B4%E6%89%B9%E6%A0%87%E5%87%86%E5%8C%96"><span class="toc-number">5.1.</span> <span class="toc-text">空间批标准化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%84%E5%BD%92%E4%B8%80%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">6.</span> <span class="toc-text">组归一化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%84%E5%BD%92%E4%B8%80%E5%8C%96%E5%8E%9F%E7%90%86"><span class="toc-number">6.1.</span> <span class="toc-text">组归一化原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">7.</span> <span class="toc-text">卷积层优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#im2col%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-number">7.1.</span> <span class="toc-text">im2col 优化技术</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5"><span class="toc-number">7.2.</span> <span class="toc-text">内存优化策略</span></a></li></ol></li></ol></div><div class="related panel pjax" data-title="系列文章"><ul><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ABatchNormalization/" rel="bookmark" title="cs231n的assignment2：BatchNormalization">cs231n的assignment2：BatchNormalization</a></li><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ADropout/" rel="bookmark" title="cs231n的assignment2：Dropout">cs231n的assignment2：Dropout</a></li><li class="active"><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ACNN/" rel="bookmark" title="cs231n的assignment2：CNN">cs231n的assignment2：CNN</a></li><li><a href="/cs231n%E7%9A%84assignment2%EF%BC%9APyTorch-on-CIFAR-10/" rel="bookmark" title="cs231n的assignment2：PyTorch on CIFAR-10">cs231n的assignment2：PyTorch on CIFAR-10</a></li></ul></div><div class="overview panel" data-title="站点概览"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="漠溟绽灵Stinnc" data-src="/images/hdd.jpg"><p class="name" itemprop="name">漠溟绽灵Stinnc</p><div class="description" itemprop="description">嘿嘿嘿~~</div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">55</span> <span class="name">文章</span></a></div><div class="item categories"><a href="/categories/"><span class="count">12</span> <span class="name">分类</span></a></div><div class="item tags"><a href="/tags/"><span class="count">133</span> <span class="name">标签</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3N0aW5uY3NreQ==" title="https:&#x2F;&#x2F;github.com&#x2F;stinncsky"><i class="ic i-github"></i></span> <span class="exturl item music" data-url="aHR0cHM6Ly9tdXNpYy4xNjMuY29tLyMvdXNlci9ob21lP2lkPTExMDY5NDgzNA==" title="https:&#x2F;&#x2F;music.163.com&#x2F;#&#x2F;user&#x2F;home?id&#x3D;110694834"><i class="ic i-cloud-music"></i></span></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>首页</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>文章</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>归档</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>分类</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>标签</a></li></ul></li><li class="item"><a href="/friends/" rel="section"><i class="ic i-heart"></i>friends</a></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/cs231n%E7%9A%84assignment2%EF%BC%9ADropout/" rel="prev" title="上一篇"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A904-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE/" rel="next" title="下一篇"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>随机文章</h2><ul><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a></div><span><a href="/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3479-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE-III/" title="力扣每日一题：3479. 水果成篮 III">力扣每日一题：3479. 水果成篮 III</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A1717-%E5%88%A0%E9%99%A4%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%9C%80%E5%A4%A7%E5%BE%97%E5%88%86/" title="力扣每日一题：1717. 删除子字符串的最大得分">力扣每日一题：1717. 删除子字符串的最大得分</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/study-notes/machine-learning/%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B/" title="自注意力">自注意力</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A904-%E6%B0%B4%E6%9E%9C%E6%88%90%E7%AF%AE/" title="力扣每日一题：904. 水果成篮">力扣每日一题：904. 水果成篮</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A808-%E5%88%86%E6%B1%A4/" title="力扣每日一题：808. 分汤">力扣每日一题：808. 分汤</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a></div><span><a href="/study-notes/machine-learning/%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/" title="批量归一化">批量归一化</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/" title="分类于 代码实现">代码实现</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="分类于 动手学深度学习">动手学深度学习</a></div><span><a href="/study-notes/machine-learning/code-implementation/PyTorch%E7%9A%84%E5%88%9D%E6%AD%A5%E4%BD%BF%E7%94%A8%EF%BC%9A%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AE%80%E5%8D%95%E6%9E%84%E5%BB%BA/" title="PyTorch的初步使用：线性模型简单构建">PyTorch的初步使用：线性模型简单构建</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A869-%E9%87%8D%E6%96%B0%E6%8E%92%E5%BA%8F%E5%BE%97%E5%88%B0-2-%E7%9A%84%E5%B9%82/" title="力扣每日一题：869. 重新排序得到 2 的幂">力扣每日一题：869. 重新排序得到 2 的幂</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/study-notes/" title="分类于 学习笔记">学习笔记</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/" title="分类于 机器学习">机器学习</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/" title="分类于 代码实现">代码实现</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/" title="分类于 cs231n">cs231n</a> <i class="ic i-angle-right"></i> <a href="/categories/study-notes/machine-learning/code-implementation/cs231n/assignment1/" title="分类于 assignment1">assignment1</a></div><span><a href="/study-notes/machine-learning/code-implementation/cs231n%E7%9A%84assignment1%EF%BC%9AImage-Features/" title="cs231n的assignment1：Image Features">cs231n的assignment1：Image Features</a></span></li><li class="item"><div class="breadcrumb"><a href="/categories/algorithm-solutions/" title="分类于 算法题解">算法题解</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/" title="分类于 力扣">力扣</a> <i class="ic i-angle-right"></i> <a href="/categories/algorithm-solutions/leetcode/daily-question/" title="分类于 每日一题">每日一题</a></div><span><a href="/algorithm-solutions/leetcode/daily-question/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%EF%BC%9A3136-%E6%9C%89%E6%95%88%E5%8D%95%E8%AF%8D/" title="力扣每日一题：3136. 有效单词">力扣每日一题：3136. 有效单词</a></span></li></ul></div><div><h2>最新评论</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; <span itemprop="copyrightYear">2025</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">漠溟绽灵Stinnc @ Stinnc</span></div><div class="count"><span class="post-meta-item-icon"><i class="ic i-chart-area"></i> </span><span title="站点总字数">192k 字</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="ic i-coffee"></i> </span><span title="站点阅读时长">2:54</span></div><div class="powered-by">基于 <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"cs231n的assignment2：CNN/",favicon:{show:"（●´3｀●）やれやれだぜ",hide:"(´Д｀)大変だ！"},search:{placeholder:"文章搜索",empty:"关于 「 ${query} 」，什么也没搜到",stats:"${time} ms 内找到 ${hits} 条结果"},valine:!0,copy_tex:!0,katex:!0,fancybox:!0,copyright:'复制成功，转载请遵守 <i class="ic i-creative-commons"></i>BY-NC-SA 协议。',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script><script src="https://fastly.jsdelivr.net/npm/hexo-tag-common@0.2.0/js/index.js"></script></body></html>